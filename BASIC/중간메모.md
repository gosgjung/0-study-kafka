# 중간메모

> 목차는 인프런 강의를 기반으로 구성.
>
> 세부적인 강의는 유튜브 강의를 더 찾아서 들으면서 정리할 예정

<br>



## 아파치 카프카 기초

### 1강 - 아파치 카프카 개요 및 설명

https://www.youtube.com/watch?v=waw0XXNX-uQ



### 2강 - 토픽이란?

https://www.youtube.com/watch?v=7QfEpRTRdIQ



### 3강 - 브로커, 복제, ISR(In-Sync-Replication)

3강 - [아파치 카프카 | Broker, Replication, ISR 👀핵심요소 3가지!](https://www.youtube.com/watch?v=qpEEoGpWVig)

<br>

가만히 앉아서 듣기만 하면 개꿀일줄 알았는데, 3강부터는 필기해야 머릿속에 기억에 남았었다... 역시 쉬운일이란 없다.<br>

<br>



프로듀서의 역할

토픽을 파티션에 전달한다.



브로커, 파티션

![1](./img/MEMO/1.png)

브로커는 여러 대 존재할 수 있다.

파티션은 브로커마다 여러대 설정할 수 있다.

리더 파티션을 설정하고, 다른 파티션을 팔로워 파티션으로 해서 복제본을 저장해두는 것 역시 가능하다.



#### 리더파티션과 팔로워 파티션의 역할

프로듀서가 토픽의 파티션에 데이터를 전달할 때 전달받는 주체가 Leader Partition 이다.

프로듀서에는 ack 라는 상세 옵션이 있다.

ack를 통해 고가용성을 유지할 수 있다.

이 옵션은 파티션의 레플리케이션과 관계있다.

ack 는 0, 1, all 이렇게 3개중 하나를 골라서 사용할 수 있다.



ack = 0 일 경우

- 프로듀서는 리더파티션에 데이터를 전송하고 응답값을 받지 않는다.

![1](./img/MEMO/2.png)

- 응답값을 받지 않기 때문에 Leader Partition 에 데이터가 정상적으로 전송됐는지, 나머지 파티션에 데이터가 정상적으로 복제되었는지 알수 없고, 보장할 수 없다.
- 이런 이유로 ack=0 일 때는 속도는 빠르지만, 데이터 유실 가능성이 있다.

![1](./img/MEMO/3.png)

ack=1 일 경우

- Leader Partition 에 데이터를 전송하고, Leader Partition 이 데이터를 정상적으로 받았는지 응답값을 받는다.
- 다만, 나머지 partition 에 복제되었는지는 알 수 없다.

![1](./img/MEMO/4.png)

- 만약, 리더 파티션이 데이터를 받은 직후 장애가 나면 나머지 partition 에 데이터가 미처 전송되지 못한 상태이기에 ack=0 옵션 처럼 데이터 유실 가능성이 있다.

![1](./img/MEMO/5.png)

ack=all 옵션

- 1 옵션에서 하는 기능에 follower partition 에 대한 기능이 추가된 기능
- follower partition 에 복제가 잘 이루어졌는지 응답값을 받는다.

![1](./img/MEMO/6.png)

- Leader partition 에 데이터를 보낸 후 나머지 Follower Partition 에도 데이터가 정상적으로 저장되는지 확인하는 절차를 거친다.

![1](./img/MEMO/7.png)

- ack=1, ack=0 일 때에 비해 확인하는 부분이 많기에 속도가 느리다는 단점 역시 존재한다.

<br>

#### replication 이 많을수록 좋은거 아니야?

![1](./img/MEMO/8.png)

하지만, 레플리케이션 갯수가 많아지면, 브로커가 사용하는 리소스 사용량도 같이 늘어나게 된다.

![1](./img/MEMO/9.png)

따라서 카프카에 들어오는 데이터 양과 retention date 즉, 저장시간을 잘 생각해서 replication 갯수를 잘 정하는 것이 좋다.<br>

3개 이상의 브로커를 사용할 때, replication 은 3으로 설정하는 것을 추천한다.

<BR>

### 4강 - 파티셔너란?

[4강 - 파티셔너의 역할과 동작! 파티션으로 가는 길목!](https://www.youtube.com/watch?v=-vKiNUH5OT8)

<br>

파티셔너를 알면 파티션을 효과적으로 사용할 수 있다.<br>

<br>

#### 파티셔너의 역할

프로듀서가 데이터를 보내면, 무조건 파티셔너를 통해서 브로커로 데이터가 전송된다.

파티셔너는 데이터를 어떤 파티션으로 넣을지 결정한다.

레코드에 포함된 메시지 키 or 메시지 값에 따라 파티션의 위치가 결정된다.

![1](./img/MEMO/10.png)





#### UniformStickyPartitioner

프로듀서를 사용할 때 파티셔너를 따로 설정하지 않는다면, UniformStickyPartitioner 로 설정된다. 

UniformStickyPatitioner 는 메시지 키가 있을때, 없을때 각각 다르게 동작한다.

<br>

##### 메시지 키가 있을 때

메시지 키를 가진 레코드는 파티셔너의 해시 알고리즘으로 파티션 번호를 해시값으로 생성할 수 있다.

동일한 메시지 키를 가진 레코드는 동일한 해시값을 만들어내기 때문에 항상 동일한 파티션에 들어가는 것을 보장한다.

![1](./img/MEMO/11.png)

<br>

이렇게 동일한 키를 가진 레코드 들은 동일한 파티션에 들어가는 것을 보장한다.

따라서 순서를 지켜서 데이터를 처리할 수 있다는 장점이 있다. 

(응? 강의에서 언급하는 이 부분은 조금 의미가 이상하다.  순서를 지킨다는 말은 좀 이상한데... FIFO와 혼동될법한데...) 

e.g. 

- 메시지 키 = 서울 => 파티션 0번 
- 메시지 키 = 울산 => 파티션 0번
- 메시지 키 = 부산 => 파티션 1번

예로 든 내용 역시 순서라기보다는 범위를 구분할 수 있다는 의미로 보인다.

(참고로, 파티션 한개일 경우에만 FIFO처럼 동작할 수 있는데, 실제로 이렇게 구성되는 케이스는 많지 않다.)

<BR>

##### e.g. 서울, 부산

![1](./img/MEMO/12.png)



#####  메시지 키가 없을 때

메시지 키가 없는 레코드는 라운드 로빈으로 파티션에 들어간다.

단, 전통적인 라운드 로빈 방식과는 조금 다르게 동작한다.

UniformStickyPartitioner 는 프로듀서에서 배치로 모을수 있는 최대한의 레코드를 모아서

파티션으로 데이터를 보내게 된다.

이렇게 배치 단위로 데이터를 보낼 때 파티션에 라운드 로빈 방식으로 돌아가면서 데이터를 넣게 된다.

<br>

쉽게 말해 메시지키가 없는 레코드 들은 

파티션에 적절하게 (내부적인 동작으로) 분배된다고 생각하면 될것 같습니다.



#### 커스텀 파티셔너

그럼 우리는 직접 개발한 파티셔너만 사용할 수 있을까요?

그렇지 않습니다.

직접 개발한 파티셔너도 우리가 프로듀서에서 설정할 수 있는데요.

카프카에서는 커스텀 파티셔너를 만들 수 있도록 `Partitioner` 인터페이스를 제공하고 있습니다.

`Partitioner` 인터페이스를 사용해서 커스텀 파티셔너 클래스를 만들면 

메시지 키 또는 메시지 값 또는 토픽 이름에 따라서 어느 파티션에 데이터를 보낼 것인지 정할 수 있습니다.



##### 커스텀 파티셔너를 사용하는 경우

e.g. VIP 고객을 위해서 데이터 처리를 조금 더 빠르게 하는 로직을 생각해볼 수 있을 것 같아요.

VIP 고객의 데이터를 조금 더 빠르게 처리해주고 싶다면

파티셔너를 통해서 처리량을 조금 더 늘릴수도 있습니다.

기본적으로 10개의 파티션이 있다고 가정할 때

우리가 커스텀 파티셔너를 만들어서

8개의 파티션에는 VIP 고객의 데이터를 저장하고

2개의 파티션에는 일반 고객의 데이터를 넣는 것



데이터 처리량을 조금 더 vip 고객을 위해서 몰아주는 형태로 개발할 수도 있습니다.

이것은 마치 AMQP 기반 메시징 시스템 같은 곳에서 우선순위 큐를 만드는 것과 비슷하다고 볼수 있습니다.

(응?... 흠...;;; )



### 5강 - 컨슈머 랙

[5강 - 카프카 컨슈머 Lag이란? Lag에 대해서 알아봅시다](https://www.youtube.com/watch?v=D7C_CFjrzBk)

<br>

카프카 lag 은 카프카를 모니터링할 때 중요하게 여겨지는 모니터링 지표 중 하나다.<br>

카프카 lag 이 존재하는 이유에 대해 아시려면 카프카 토픽과 파티션, 컨슈머와 프로듀서, 오프셋에 대해 모두 아셔야 합니다.<br>

만약 이 5가지에 대해서 모르신다면 [데이터💾가 저장되는 토픽에 대해서 알아봅시다.](https://www.youtube.com/watch?v=7QfEpRTRdIQ) 을 눌러서 보고 오시는 것을 추천드립니다.<br>

카프카 프로듀서는 토픽의 파티션에 데이터를 차곡 차곡 넣게 됩니다.<br>

이 파티션에 데이터가 하나 하나 들어가게 되면 각 데이터는 오프셋이라고 하는 숫자가 붙게 됩니다.<br>

<br>

e.g. 파티션이 1개일 때

만약 파티션이 1개인 토픽에 프로듀서가 데이터를 넣을 경우 0부터 차례대로 숫자가 매겨지게 됩니다.

프로듀서는 계속해서 데이터를 넣게 되고

컨슈머는 계속해서 데이터를 가져간다.

![1](./img/MEMO/13.png)



만약 프로듀서가 데이터를 넣어주는 속도가 컨슈머가 가져가는 속도보다 빠르게 되면 어떻게 될까요?

- 프로듀서가 넣은 데이터의 오프셋
- 컨슈머가 가져간 데이터의 오프셋

프로듀서의 오프셋, 컨슈머의 오프셋 이렇게 두개의 오프셋 간에 차이가 발생하게 된다.

![1](./img/MEMO/14.png)



이 lag은 적을수도 있고 많을 수도 있습니다.

이 lag의 숫자를 통해 현재 해당 토픽에 대해 파이프라인으로 연결되어 있는 프로듀서와 컨슈머의 상태에 대해 유추가 가능하게 되는데요.

주로 컨슈머의 상태에 대해 볼때 사용합니다.



lag 은 각 파티션의 오프셋 기준으로 프로듀서가 넣은 데이터의 오프셋과

컨슈머가 가져가는 데이터의 오프셋이 차이를 기반으로 합니다.



그렇기 때문에 토픽에 여러 파티션이 존재할 경우 lag은 여러개가 존재할 수 있습니다.

<br>

만약 컨슈머 그룹이 1개이고 파티션이 2개인 토픽에서 데이터를 가져간다면 lag은 2개가 측정될 수 있습니다.

![1](./img/MEMO/15.png)



이렇게 한개의 토픽과 컨슈머 그룹에 대한 lag이 여러개 존재할 수 있을 때

그 중 높은 숫자의 lag을 `records-lag-max` 라고 부릅니다.



오늘은 카프카 렉에 대해 알아봤습니다. 저는 현업에서 컨슈머 입장으로 개발을 많이 해왔기 때문에 lag에 대해 많은 모니터링 경험이 있습니다. 아무래도 consumer가 성능이 안나오거나 비정상적인 동작을 하게 되면 lag이 필연적으로 발생하기 때문에 주의깊게 살펴볼 needs 가 있기 때문입니다.



lag은 두가지만 알면 됩니다.

- 첫번째, lag은 프로듀서의 오프셋과 컨슈머의 오프셋 간의 차이다.
- 두번째, lag은 여러개가 존재할 수 있다.



### 6강 - 카프카 컨슈머 Lag 모니터링 필수 요소

- Burrow 소개 : [https://blog.voidmainvoid.net/243](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjU1UUw5OGdRd2QyQXRDOTlkenhnSFNwb2ZvQXxBQ3Jtc0tsb3F6bk92OUh6a3ctc1lwclkwNVF5UXU1Q3pZYTRpdU9hczZYc2lITDVMX3RWTGt6MERCdDVBbnVZNXJXUXZ4czhKeFNjRHlkX0g0MHVfZVp4RnBSN21mX1BLZGNUTWY3ZlFzYXgxV1FrMHZCYmxocw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F243&v=b3i6D4eeBGw) 
- Burrow의 Consumer status 확인 방법 : [https://blog.voidmainvoid.net/244](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFltMHBrZFczVkVQQkU5d1NLbHNTZVMydVBEd3xBQ3Jtc0tuZEhyaVQ0VWlnVTEyT3E0RDhIaWdRME1NWHBWUmRFQlRQMURrLUo0Y2Jad1ZjNzJrYnpfWlpxQ0VjekVMOWR2akxZRW9FLTB4cVhxS1ViWDRLaDloQV9SVENmSjFDZmNkcTBJeWRFaUZUUTFYWjFkUQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F244&v=b3i6D4eeBGw) 
- Burrow http endpoint 정리 : [https://blog.voidmainvoid.net/245](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJINUhicHptcnhRNlFwVmxBclBXODZtQTZuQXxBQ3Jtc0trYWxjd0RCVDJBYWVTcXRVWklmanJpWVl6dkJvTk4wU2RGUWQwa1dLRmgyUDFvTk9vZndjNmRhZHU1cXl5MWNJRmNnQThOYzRZM0pkXzFkbXZmcElUa1hyMmVYZFk4QWNtaGRKa005RWY1TEhhREhySQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F245&v=b3i6D4eeBGw) 
- Burrow github : [https://github.com/linkedin/Burrow](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUZOMXdwRjB3STFTcTM1MEQ4Z3FreTR2eWFwQXxBQ3Jtc0ttVjJtUUZrWFZueDMtdHNMZ1RXcnVDeFlILXpPa2NLOTYycGFmU1FnN2I5RTNyOHJ3TGpmaldCTHhLbXFGMzJnN2xJQnpDdzZUanNHZTdVLURKM0xHaTNyQ3NocU9PbU5QX014QWlFbmE0ZU41R2Vzbw&q=https%3A%2F%2Fgithub.com%2Flinkedin%2FBurrow&v=b3i6D4eeBGw)



오늘은 카프카 lag을 모니터링하기 위해 오픈소스인 Burrow 를 사용해야 하는 이유에 대해 말씀드리려 합니다.

컨슈머 Lag을 모니터링해야 하는 이유에 대해서는 

저번 영상에서 이미 말씀드렸었는데요.

아직 지난 영상을 보지 않으셨다면 [카프카 컨슈머 Lag이란? Lag에 대해서 알아봅시다](https://www.youtube.com/watch?v=D7C_CFjrzBk) 를 보고 오시는 것을 추천드립니다.



카프카 lag 은 토픽의 가장 최신 오프셋과 컨슈머 오프셋 간의 차이입니다.

![1](./img/MEMO/16.png)



kafka-client 라이브러리를 사용해 Java/Scala 같은 언어를 통해 카프카 컨슈머를 구현할 수 있는데요.

이때 구현한 카프카 컨슈머 객체를 통해 현재 lag 정보를 가져올 수 있습니다.

만약 lag을 실시간으로 모니터링하고 싶다면

데이터를 Elasticsearch 나 influxDB와 같은 저장소에 넣은 뒤

Grafana 대시보드를 통해 확인할 수도 있습니다.

![1](./img/MEMO/17.png)





그런데 문제는 이렇게 Consumer 단위에서 lag을 모니터링하는 것은 

아주 위험하고 운영요소가 많이 들어간다는 점입니다.

왜냐면 컨슈머 로직단에서 lag을 수집하는 것은

컨슈머 상태에 디펜던시가 걸리기 때문입니다.



컨슈머가 비정상적으로 종료된다면, 컨슈머는 더 이상 lag 정보를 보낼 수 없기 때문에

더이상 lag 을 측정할 수 없습니다.



그리고 추가적으로 컨슈머가 개발될 때마다 

해당 컨슈머에 lag 정보를 특정 저장소에 저장할 수 있도록 

로직을 개발해야 합니다.



만약 컨슈머 lag을 수집할 수 없는 컨슈머라면

lag을 모니터링할 수 없으므로 

운영이 매우 까다로워지게 됩니다.



그래서 linkedin 에서는 아파치 카프카와 함께

카프카 컨슈머 lag을 효과적으로 모니터링할 수 있도록

Burrow 를 내놓았습니다.



Burrow 는 오픈소스로서 Golang으로 작성되었고 현재 깃헙에 올라와있습니다.

가장 최근 릴리즈일자를 찾아보면 1월 29일 1.3.2 버전을 배포한 것을 알 수 있는데요

지속적으로 관리되고 있는 오픈소스라는 것을 알 수 있습니다.



Burrow 는 컨슈머 lag 모니터링을 도와주는 

독립적인 애플리케이션이라고 보시면 됩니다.



Burrow 는 3가지 큰 특징을 가지고 있는데요

- 1\) 멀티 카프카 클러스터 지원
  - 다양한 유즈케이스가 있을 수 있겠지만, 카프카를 운영하는 기업이라면 대부분이 2개 이상의 카프카 클러스터를 운영하고 있을 것입니다.
  - 이렇게 카프카 클러스터가 여러개이더라도 Burrow application 1개만 실행해 연동한다면 카프카 클러스터들에 붙은 컨슈머의 lag을 모두 모니터링할 수 있습니다. (참고: 아래 그림)
- 2\) Sliding window 를 통한 Consumer 의 status 확인
  - 만약 데이터 양이 일시적으로 많아지면서 consumer offset 이 증가되고 있으면 'WARNING'으로 정의됩니다.
  - 만약 데이터 양이 많아지고 있는 consumer 가 데이터를 가져가지 않으면 'ERROR'로 정의해서 실제로 컨슈머가 문제가 있는지 알 수 있습니다.
  - 이렇게 status 를 기반으로 효과적으로 운영에 참고할 수 있습니다.
- 3\) HTTP API 제공
  - 위와 같은 정보들은 Burrow 가 정의한 HTTP API 를 통해 조회할 수 있게 했습니다.
  - 세상에는 여러 프로토콜이 있지만, 그 중에 가장 범용적으로 사용되는 HTTP를 제공한 덕분에 Burrow 는 다양한 추가 생태계를 구축할 수 있게 되었습니다.
  - HTTP API를 호출해서 response 받은 데이터를 시계열 DB와 같은 곳에 저장하는 application 을 만들어서 활용할 수도 있습니다.



Burrow Application 과 카프카 클러스터들간의 관계 

![1](./img/MEMO/18.png)





아파치 카프카를 개발한 Linkedin 개발자들은 

컨슈머 lag을 어떻게 모니터링할지 많은 고민을 해왔다는 것을

Linkedin Engineering 블로그에서도 확인할 수 있습니다.



결국 아파치 카프카 외부 application 을 통해 

컨슈머 lag을 모니터링하는 것이 답이라는 것을 알게 되었고

오픈소스화된 Burrow는 현재 많은 데이터 기반 기업들이 사용 중에 있습니다.



아파치 카프카를 운영하는 기업이라면 Burrow 를 도입하지 않을 이유가 없습니다.



물론, Burrow 를 도입한다고 모든 문제가 해결되는 것은 절대 아닙니다.

다만 카프카 개발자 그리고 카프카 클러스터 운영자가 

효과적으로 카프카 관련 애플리케이션을 운영할 때 

반드시 필요한 것이고

Burrow 를 통해 수집된 데이터는

결국 추후에 애플리케이션 개발과 운영시에 많은 도움이 되기 때문입니다.



만약 이 영상을 보시는 구독자님들 중에 

회사에 아파치 카프카를 사용하고 있는데 아직 Burrow 를 도입하지 않으셨다면

이번 기회에 Burrow 를 도입하는 것을 고려해보시는 것도 

정말 좋을 것 같습니다.



- Burrow 소개 : [https://blog.voidmainvoid.net/243](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjU1UUw5OGdRd2QyQXRDOTlkenhnSFNwb2ZvQXxBQ3Jtc0tsb3F6bk92OUh6a3ctc1lwclkwNVF5UXU1Q3pZYTRpdU9hczZYc2lITDVMX3RWTGt6MERCdDVBbnVZNXJXUXZ4czhKeFNjRHlkX0g0MHVfZVp4RnBSN21mX1BLZGNUTWY3ZlFzYXgxV1FrMHZCYmxocw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F243&v=b3i6D4eeBGw) 
- Burrow의 Consumer status 확인 방법 : [https://blog.voidmainvoid.net/244](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFltMHBrZFczVkVQQkU5d1NLbHNTZVMydVBEd3xBQ3Jtc0tuZEhyaVQ0VWlnVTEyT3E0RDhIaWdRME1NWHBWUmRFQlRQMURrLUo0Y2Jad1ZjNzJrYnpfWlpxQ0VjekVMOWR2akxZRW9FLTB4cVhxS1ViWDRLaDloQV9SVENmSjFDZmNkcTBJeWRFaUZUUTFYWjFkUQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F244&v=b3i6D4eeBGw) 
- Burrow http endpoint 정리 : [https://blog.voidmainvoid.net/245](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJINUhicHptcnhRNlFwVmxBclBXODZtQTZuQXxBQ3Jtc0trYWxjd0RCVDJBYWVTcXRVWklmanJpWVl6dkJvTk4wU2RGUWQwa1dLRmgyUDFvTk9vZndjNmRhZHU1cXl5MWNJRmNnQThOYzRZM0pkXzFkbXZmcElUa1hyMmVYZFk4QWNtaGRKa005RWY1TEhhREhySQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F245&v=b3i6D4eeBGw) 
- Burrow github : [https://github.com/linkedin/Burrow](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUZOMXdwRjB3STFTcTM1MEQ4Z3FreTR2eWFwQXxBQ3Jtc0ttVjJtUUZrWFZueDMtdHNMZ1RXcnVDeFlILXpPa2NLOTYycGFmU1FnN2I5RTNyOHJ3TGpmaldCTHhLbXFGMzJnN2xJQnpDdzZUanNHZTdVLURKM0xHaTNyQ3NocU9PbU5QX014QWlFbmE0ZU41R2Vzbw&q=https%3A%2F%2Fgithub.com%2Flinkedin%2FBurrow&v=b3i6D4eeBGw)



### 7강 - 카프카, 레빗엠큐, 레디스 큐의 차이점

[7강 - 카프카, 레빗엠큐, 레디스 큐의 큰 차이점! 이벤트 브로커와 메시지 브로커에 대해 알아봅시다.](https://www.youtube.com/watch?v=H_DaPyUOeTo)



흔히 말하는 이 메시징 플랫폼이라고 부르는 것들은 사실

두가지 종류로 나누어지는데요.



첫번째는 메시지 브로커이고

두번째는 이벤트 브로커에요



이 둘의 관계에 대해서 짧게 설명드리자면

메시지 브로커는 이벤트 브로커로 역할을 할 수 없지만

이벤트 브로커는 메시지 브로커로 역할을 할 수 있습니다.



이 설명으로는 아마 충분히 이해하기 어려울 수 있어요



먼저, 첫번째로 메시지 브로커부터 알아볼께요

우선 메시지 브로커는 많은 기업들에서 대규모 메시지 기반 미들웨어 아키텍처에서 사용되어 왔습니다.

미들웨어라는 것은 서비스하는 애플리케이션들을 

보다 효율적으로 아키텍처들을 연결하는 요소들로 작동하는 소프트웨어를 뜻하는데요.



메시징 플랫폼, 인증 플랫폼, 데이터베이스 를 미들웨어라고 볼 수 있어요

메시지 브로커에 있는 큐에 데이터를 보내고 받는 

프로듀서와 컨슈머를 통해 메시지를 통신하고 네트워크를 맺는 용도로 사용해왔죠.



메시지 브로커의 특징으로는

메시지를 받아서 적절히 처리하고 나면

즉시 또는 짧은 시간 내에 삭제되는 구조입니다.



반면에 이벤트 브로커는 메시지 브로커의 특징과

조금 다른 구조로 만들어져 있는데요.



두가지 큰 특징이 있어요



첫번째는 이벤트 또는 메시지라고 불리는 이 레코드 이 장부를 딱 하나만 보관하고 

인덱스를 통해 개별 액세스를 관리합니다.



두번째는 업무상 필요한 시간동안 이벤트를 보존할 수 있습니다.

확실히 메시지 브로커랑 결이 다른 것이 느껴지시나요?



메시지 브로커는 데이터를 보내고 처리하고 삭제한다.

그런데, 이벤트 브로커는 삭제하지 않아요



아니, 데이터를 처리했는데 왜 삭제하지 않는 걸까요?

단서는 '이벤트'라는 단어에 숨어있습니다.



이벤트 브로커는 서비스에서 나오는 이벤트를 

마치 데이터베이스에 저장하듯이

이벤트 브로커의 큐에 저장하는 데요

이렇게 저장함으로써 얻는 명확한 이점이 있습니다.



첫 번째 이점은 딱 한번 일어난 이벤트 데이터를 브로커에 저장함으로써 단일 진실 공급원으로 사용할 수 있구요.

두 번째는 장애가 발생했을 때, 장애가 일어난 지점부터 재처리할 수 있습니다.

그리고 세번째로는 많은 양의 실시간 스트림 데이터를 효과적으로 처리할 수 있다는 특징이 있어요.



그 외에도 다양한 이벤트 기반 마이크로 서비스 아키텍처에서 중요한 역할을 맡을 수 있습니다.

메시지 브로커는 보통 레디스 큐나 레빗엠큐 같은 것을 예로 들 수 있고

이벤트 브로커는 카프카나 AWS의 키네시스가 대표적이겠죠.



이벤트 브로커로 클러스터를 구축하면

이벤트 기반 마이크로 서비스 아키텍처로 발전하는데

아주 중요한 역할을 할 뿐만 아니라

메시지 브로커로서도 사용할 수 있으니까

정말 팔방 미인이 아닐 수 없어요



여기까지 이벤트 브로커와 메시지 브로커에 대한 차이점

그리고 각각의 특징에 대해서 간략하게 살펴보았는데요.



## 아파치 카프카 개발

### 8강 - AWS에 카프카 클러스터 설치, 실행하기

[8강 - AWS에 카프카 클러스터를 설치, 실행해보기!](https://www.youtube.com/watch?v=Qr0HVvtMFhg)

자세한 설치 과정은 [블로그](https://blog.voidmainvoid.net/325)를 참고<br>

<br>

이 영상을 보시는 많은 개발자 분들은 이미 자신의 컴퓨터에 1개의 node로 카프카 브로커를

homebrew를 통해 설치해보셨을 것 같은데요.

사실 3개 이상의 카프카 브로커로 이루어진 클러스터를 구축하고 사용해야지만

정말로 카프카를 써봤다 라고 말할 수 있을 것 같습니다.

왜냐면 카프카의 고가용성의 핵심은 3개 이상의 카프카 브로커들로 이루어진

클러스터에서 진가를 발휘하게 되거든요



그래서 오늘은 aws 의 ec2 서버 3대를 발급받아서

카프카를 설치해보고 console producer와 console consumer 로 

연동해보는 실습을 해보도록 하겠습니다.



먼저 아파치 카프카를 설치하기 위해서는 2가지의 애플리케이션이 필요한데요.

먼저 첫번째는 주키퍼이고, 두번째는 카프카입니다.

주키퍼는 카프카 관련 정보를 저장하는 역할을 하는데요

카프카에서 주키퍼의 역할에 대해서는 다른 영상에서 자세히 설명드리도록 하겠습니다.



그럼 한번 카프카와 주키퍼 설치를 진행해보도록 하겠습니다.



테스트목적의 머신이므로 1cpu, 1g ram 으로 구성된 t2.micro 를 발급받습니다.

![1](./img/MEMO/19.png)



카프카 클러스터를 최소로 구축하기 위해 3대의 인스턴스를 발급받습니다.

![1](./img/MEMO/20.png)



키페어 다운로드

![1](./img/MEMO/21.png)



이제 인스턴스 시작 버튼을 눌러 서버가 실행될 때 까지 기다립니다.

![1](./img/MEMO/22.png)



인스턴스가 모두 시작되고 나면 인스턴스에 접속하셔서 wget 명령어로 주키퍼를 다운받습니다.

![1](./img/MEMO/23.png)



다운받은 주키퍼의 압축을 풀어줍니다.

![1](./img/MEMO/24.png)



주키퍼 앙상블을 구축하기 위해서는 각 서버마다 주키퍼 설정을 하도록 합니다.

![1](./img/MEMO/25.png)



자세한 주키퍼 설정은 영상하단의 블로그 링크에서도 확인하실 수 있습니다.

![1](./img/MEMO/26.png)



각 서버별로 ip가 아닌 hostname으로 통신하기 위해 /etc/host 를 수정해줍니다.

![1](./img/MEMO/27.png)



주키퍼를 서로 연동하기 위해서는 방화벽 설정이 필요한데요

방화벽 설정은 시큐리티 그룹의 inbound rule과 outbound rule 을 통해 설정 가능합니다.

zookeeper 는 2181 포트와 2888포트, 3888 포트를 사용하므로

3개의 포트에 대해서 anywhere 조건으로 open 하도록 하겠습니다.



인바운드 방화벽 설정을 수정합니다.

![1](./img/MEMO/28.png)



참고로 카프카 통신을 위해 9092 포트도 열어주시면 좋습니다.

![1](./img/MEMO/29.png)



이제 설정이 완료되었으니 각 서버에서 주키퍼를 실행합니다.

![1](./img/MEMO/30.png)



주키퍼가 정상적으로 설치되었고 방화벽이 정상이라면

local 컴퓨터의 주키퍼 cli 를 통해

aws 에서 실행되는 주키퍼에 연결하실 수 있습니다.

![1](./img/MEMO/32.png)



이제 카프카를 설치해보도록 하겠습니다.

이번에 설치할 카프카는 2.1.0 버전인데요.

가장 최근에 나온 버전인 2.4.0 보다 좀 더 낮은 버전입니다.



카프카 압축 파일을 인터넷에서 다운로드합니다.

![1](./img/MEMO/33.png)





카프카 클러스터를 구성하기 위해 각 브로커 별로 카프카 설정이 필요합니다.

![1](./img/MEMO/34.png)



가장 중요한 broker.id 를 각 서버별로 각각 다른 숫자로 설정해줍니다.

listener 와 advertise listener 도 설정합니다.(그림 생략)

![1](./img/MEMO/35.png)



그리고 아까 실행한 주키퍼의 hostname과 port 도 넣어주도록 합니다.

![1](./img/MEMO/36.png)



이제 카프카를 실행해보도록 하겠습니다.

![1](./img/MEMO/37.png)





카프카 실행 된 후 테스트를 위해 `test_log` 라고 하는 토픽을 만들어보겠습니다.

![1](./img/MEMO/38.png)



그리고 만들어진 토픽에 console-producer 로 데이터를 넣습니다.

![1](./img/MEMO/39.png)



동시에 console-consumer로 데이터를 확인합니다.

![1](./img/MEMO/40.png)



AWS에 설치된 카프카와 정상적으로 통신하는 것을 확인할 수 있습니다.

![1](./img/MEMO/41.png)



여기까지 카프카 클러스터를 멀티 브로커로 구축해보고

local 컴퓨터에서 카프카 클러스터로

연동해보는 실습까지 완료했습니다.



이렇게 만들어진 카프카 클러스터는 3개의 브로커로 이루어져 있기 때문에

고 가용성을 만족한다고 볼 수 있고

고 가용성을 만족하는 훌륭한 클러스터를 

직접 만드셨다는 것에 자부심을 가져도 될 것 같습니다.



오늘 설치과정은 [블로그](https://blog.voidmainvoid.net/325) 에서 확인할 수 있습니다.



### 9강 - 카프카 프로듀서 애플리케이션

[9강 - 데이터를 카프카로 전송🚀하는 프로듀서](https://www.youtube.com/watch?v=aAu0FE3nvbk)

<br>

프로듀서는 데이터를 카프카에 보내는 역할을 합니다. 예를 들어 엄청난 양의 클릭로그들을 

대량으로, 그리고 실시간으로 카프카에 적재할 때 

프로듀서를 사용할 수 있습니다.



프로듀서는 이름에서 알수 있는 것처럼 데이터를 프로듀싱, 즉, 생산하는 역할을 합니다.

데이터를 카프카 토픽에 생산한다는 말이죠

![1](./img/MEMO/42.png)



프로듀서는 다음과 같은 역할을 하는 데요.

첫번째로 토픽에 전송할 메시지를 생성하는 역할을 합니다.

두번째로는 특정 토픽으로 데이터를 전송(PUBLISH)할 수 있습니다.

이를 통해 기본적인 카프카 데이터 전송이 완성 됩니다.

그리고 마지막으로 카프카 브로커로 데이터를 전송할 때 전송 성공/실패 여부를 알 수 있고 실패할 경우 재시도할 수도 있습니다.

![1](./img/MEMO/43.png)



카프카 클라이언트인 컨슈머와 프로듀서를 사용하기 위해서는 

아파치 카프카 라이브러리를 추가해야 합니다.

라이브러리는 그레이들이나 메이븐 같은 도구를 사용하여 편리하게 가져올 수 있습니다.

카프카 클라이언트를 의존성으로 잡을 때 주의하실 점은 바로 버전입니다.

카프카는 브로커의 버전과 클라이언트의 버전의 하호환성이 완벽하게 모든 버전에 대해 지원되지는 않습니다.

일부 카프카 브로커 버전은 특정 카프카 클라이언트 버전을 지원하지 않을 수도 있습니다.

그러므로 브로커와 클라이언트 버전의 하위 호환성을 확인하시고 이에 알맞는 카프카 클라이언트 버전을 사용하셔야 합니다.

클라이언트 버전 별 하위호환성에 대한 설명은 영상 아래에 블로그 링크를 달아두었으니 참고하시면 됩니다.

![1](./img/MEMO/44.png)



카프카 프로듀서 코드를 보겠습니다.

자바 프로퍼티 객체를 통해 프로듀서의 설정을 정의하는 데요

![1](./img/MEMO/45.png)

- bootstrap.servers : localhost:9002
  - 부트스트랩 서버 설정을 로컬호스트의 카프카를 바라보도록 설정
  - 주소 목록은 되도록 두개 이상의 아이피/포트를 설정하도록 권장되고 있습니다.
  - 둘중 한개 브로커가 비정상일 경우 다른 정상적인 브로커에 연결되어 사용가능하기 때문입니다.
  - 그러므로 실제로 애플리케이션을 카프카와 연동할 때는 반드시 2개 이상의 브로커 정보를 넣는 것을 추천드립니다.
- key.serializer, value.serializer
  - key.serializer, value.serializer 모두 StringSerializer 를 사용한다.
  - 시리얼라이저는 키 또는 밸류를 직렬화하기 위해 사용된다.
  - 직렬화의 종류로는 ByteArray, String, Integer 시리얼라이즈가 있다.
  - 키
    - 메시지를 보낼 때 킷값을 통해 토픽의 파티션이 지정될 때 사용된다.



카프카 프로듀서의 인스턴스를 생성했다.

![1](./img/MEMO/46.png)



전송할 객체를 만들어야 하는데, 카프카 클라이언트에서는 ProducerRecord 클래스를 제공합니다.

ProducerRecord 인스턴스를 생성할 때 어느 토픽에 넣을 것인지 어떤 key,value 를 담을 것인지 선언할 수 있습니다.

아래 코드는 key 없이 click_log 토픽에 login 이라는 value를 보냅니다.

![1](./img/MEMO/47.png)



만약 key 를 포함하여 보내고 싶다면 아래코드와 같이 ProducerRecord 를 선언하면 됩니다.

파라미터 갯수에 따라 자동으로 오버로딩된 메서드가 선택되므로 이점을 유의해서 ProducerRecord를 생성하셔야 합니다.

![1](./img/MEMO/48.png)



데이터가 도착할 토픽, 데이터, 카프카 브로커의 호스트와 포트까지

데이터를 전송할 모든 준비가 되었습니다.

![1](./img/MEMO/49.png)



이전에 생성된 프로듀서 인스턴스에

send() 메서드의 파라미터로 ProducerRecord를 넣으면

전송이 이루어지게 됩니다.

click_log 토픽에 login 밸류가 들어가게 되죠 

전송이 완료되면 close 메서드로 전송을 종료합니다.

![1](./img/MEMO/50.png)



키가 null 인 데이터를 파티션이 하나인 토픽에 보내면 아래와 같이 차례대로 쌓입니다.

![1](./img/MEMO/52.png)



파티션이 하나 더 늘어나면 어떻게 될까요?

키가 null 이므로 데이터가 round-robin 으로 2개의 파티션에 차곡 차곡 쌓이게 됩니다.

![1](./img/MEMO/53.png)



반면에 키가 존재하는 데이터를 토픽에 보내면 어떻게 될까요?

buy 라는 밸류의 키를 "1", review 라는 밸류의 키를 "2"로 지정해봤습니다.

kafka 는 key 를 특정한 hash값으로 변경시켜

파티션과 1대1 매칭을 시킵니다.

그러므로 위 코드를 반복해서 실행시키면

각 파티션에 동일 키의 밸류만 쌓이게 되는 것이죠

![1](./img/MEMO/54.png)



만약 여기서 파티션을 하나 더 추가하게 된다면, 어떻게 될까요>

토픽에 새로운 파티션을 추가하는 순간

key와 파티션의 매칭이 깨지기 때문에 key와 파티션의 연결은 보장되지 않습니다.

그러므로 key 를 사용할 경우 이점을 유의해서 파티션 개수를 생성하고

추후에 생성하지 않는 것을 추천드립니다.

![1](./img/MEMO/55.png)



카프카 프로듀서의 기본코드의 구현은 앞서 보신것처럼 매우 간단합니다.

그러나 데이터 유실 혹은 브로커의 이슈에 대처하기 위해

추가적인 옵션들과 코드가 필요합니다.

이러한 추가옵션과 이슈 대응 코드는 다음 영상으로 설명드리겠습니다.





### 10강 - 카프카 컨슈머 애플리케이션

[10강 - 카프카 컨슈머 역할 및 코드예제](https://www.youtube.com/watch?v=rBVCvv9skT4)

참고 : [아파치 카프카 버전별 하위호환](https://blog.voidmainvoid.net/193)

<br>



카프카 컨슈머의 동작은 다른 메시징 플랫폼의 동작과는 다른데요.

다른 메시징 플랫폼에서 컨슈머가 데이터를 가져가게 되면 데이터가 사라지는데요.

하지만 카프카에서는 컨슈머가 데이터를 가져가더라도

데이터가 사라지지 않습니다.



이와 같은 특징은 카프카 그리고 카프카 컨슈머를 데이터 파이프라인으로 운영하는데에 매우 핵심적인 역할을 하는데요

카프카 컨슈머가 데이터 파이프라인으로서 어떻게 동작하는지

그리고 카프카 컨슈머의 역할에 대해 알고 싶으시다면 이 영상을 봐주세여



카프카 컨슈머는 기본적으로 토픽의 데이터를 가져옵니다.

이전 영상에서 말씀드렸다시피 데이터는 토픽 내부의 파티션에 저장되는 데요

컨슈머는 이렇게 파티션에 저장된 데이터를 가져옵니다. 

이렇게 데이터를 가져오는 것을 폴링(polling)이라고 합니다.

만약 토픽이나 파티션에 대해 잘 모른다면 [이전 영상](https://www.youtube.com/watch?v=7QfEpRTRdIQ)을 참고해주세요.

![1](./img/MEMO/56.png)



컨슈머의 역할은 크게 세가지로 볼 수 있습니다.

첫번째는 토픽 내부의 파티션에서 메시지를 가져오는 것 입니다.

메시지를 가져와서 특정 데이터베이스에 저장하거나

또 다른 파이프라인에 전달할 수 있습니다.



두번째로 오프셋 위치를 커밋할 수 있습니다.

여기서 오프셋이란 파티션에 있는 데이터의 번호를 의미합니다.

이 오프셋 커밋 관련 부분은 이후에 그림과 함께 설명드리도록 하겠습니다.



마지막으로 컨슈머가 여러개일 경우 병렬 처리를 할 수 있습니다.

파티션 갯수에 따라 컨슈머를 여러개 만들면 병렬 처리가 가능하기 때문에 더욱 빠른 속도로 데이터를 처리할 수 있습니다.

![1](./img/MEMO/57.png)



컨슈머를 사용하기 위해서는 의존성을 추가해야 합니다.

프로듀서와 마찬가지로 카프카 브로커와의 버전차이가 있기 때문에 

반드시 호환되는 버전인지 확인 후 사용하는 것을 권장합니다.

버전과 관련해서는 [아파치 카프카 버전별 하위호환](https://blog.voidmainvoid.net/193) 을 참고해주세요

![1](./img/MEMO/59.png)



컨슈머를 사용하기 위해 먼저 자바 프로퍼티를 설정해야 합니다.

- "bootstrap.servers"
  - 카프카 브로커를 여러개를 설정해서, 하나의 브로커에 문제가 생기면 다른 브로커들이 제 역할을 할 수 있도록 여러개의 브로커를 설정하는 것을 추천
- "group.id"
  - 컨슈머 그룹. 컨슈머들의 묶음
- key.serializer, value.serializer
  - 직렬화 설정

![1](./img/MEMO/61.png)



consumer 인스턴스 생성

컨슈머 인스턴스를 이용해 데이터를 읽고 처리하는 것이 가능하다.

![1](./img/MEMO/62.png)



컨슈머 그룹을 정한다.

어느 카프카 브로커에서 데이터를 가지고 올지 선언했기 때문에

이제 어느 토픽을 대상으로 데이터를 가져올지 선언해야 한다.

![1](./img/MEMO/63.png)



어느 토픽에서 데이터를 가져올 지는

consumer의 subscribe() 메서드를 통해 설정할 수 있다.

만약 특정 토픽의 전체 파티션이 아니라

일부 파티션의 데이터만 가져오고 싶다면 assign 메서드를 사용하면 됩니다.

키가 존재하는 데이터라면 이 방식을 통해 데이터의 순서를 보장하는 데이터 처리를 할 수 있습니다.

![1](./img/MEMO/64.png)



이제 데이터를 실질적으로 가져오는 폴링 루프 구문에 대해 설명드리겠습니다.

참고로 카프카 컨슈머에서 폴링 루프는 poll() 메서드가 포함된 무한루프를 말합니다.

컨슈머 API의 핵심은 브로커로부터 연속적으로 

그리고 컨슈머가 허락하는 한 많은 데이터를 읽는 거신데오

이런 측면에서 폴링 루프는 컨슈머 api 의 핵심로직이라고 보실 수 있습니다.

컨슈머는 poll() 메서드를 이용해 데이터를 가져오는데요.

poll() 메서드에서 설정한 시간 동안 데이터를 기다리게 됩니다.

아래 예제에서는 500ms(0.5초) 동안 데이터가 도착하기를 기다리고 

이후에 코드를 실행합니다.

만약 0.5초 동안 데이터가 들어오지 않으면 빈 값의 records 변수를 반환하고

데이터가 있다면 데이터가 존재하는 records 값을 반환합니다.

records 변수는 데이터 배치로서 레코드의 묶음 list 입니다.

그러므로 실제로 카프카에서 데이터를 처리할 때는 가장 작은 단위인 record 로 나누어 처리하도록 합니다.

record.value() 로 받아온 값이 producer 가 전송한 데이터라고 보시면 됩니다.

![1](./img/MEMO/66.png)



이번 코드에서는 폴링 루프 구문에서 특정 topic 으로부터 가져온 데이터를 println 으로만 찍어보기만 했는데

실제 기업에서는 데이터를 하둡, 엘라스틱 서치와 같은 저장소에 저장하는 로직을 넣기도 합니다.



Producer.send()

프로듀서는 키가 null 일 경우 가지고 있는 파티션 들에 라운드 로빈으로 데이터를 넣습니다.

이렇게 파티션에 들어간 데이터는 파티션 내에서 고유한 번호를 가지게 되는데 이 번호를 offset 이라고 부릅니다.

![1](./img/MEMO/67.png)



오프셋은 투픽별로 그리고 파티션 별로 별개로 지정됩니다.

![1](./img/MEMO/68.png)



이 offset 이하는 역할은 컨슈머가 데이터를 어느 지점까지 읽었는지 확인하는 용도로 사용됩니다.

컨슈머가 데이터를 읽기 시작하면 offset 을 커밋하게 되는데

이렇게 가져간 내용에 대한 정보는 카프카의 `__consumer_offset` 토픽에 저장합니다.

![1](./img/MEMO/69.png)



consumer 는 파티션이 2개인 `click_log` 토픽에서 데이터를 가져가게 됩니다.

이렇게 가져갈 때 마다 offset 정보가 저장되게 됩니다.

그런데 만약 컨슈머가 불의의 사고로 실행이 중지되었다고 가정해봅시다.

![1](./img/MEMO/70.png)

이 컨슈머가 어디까지 데이터를 읽었는지에 대한 정보는 `__consumer_offset` 에 저장되어 있습니다.

따라서 이 컨슈머를 재실행하면 중지되었던 시점을 알고 있으므로 

시작 위치부터 다시 복구하여 데이터 처리를 할 수 있습니다.

<br>

![1](./img/MEMO/71.png)



컨슈머에 이슈가 발생하더라도 데이터의 처리 시점을 복구할 수 있는 고 가용성의 특징을 가지게 되는 것이죠.

![1](./img/MEMO/72.png)

<br>

그럼 이러한 컨슈머는 몇개까지 생성할 수 있을까요?

만약 컨슈머가 한개인 경우에는 2개의 파티션에서 데이터를 가져갑니다.

![1](./img/MEMO/73.png)



2개의 컨슈머의 경우에는 각 컨슈머가 각각의 파티션을 할당하여 데이터를 가져가서 처리합니다. 

![1](./img/MEMO/74.png)



만약 3개의 컨슈머라면 어떨까요?

이미 파티션들이 각 컨슈머에 할당되었기 때문에  더 할당될 파티션이 없어서 동작하지 않습니다.

![1](./img/MEMO/75.png)



이와 같이 여러 파티션을 가진 토픽에 대해서 컨슈머를 병렬처리하고 싶다면

반드시 컨슈머를 파티션 갯수보다 적은 갯수로 실행시켜야 한다는 점을 꼭 기억해주세요.



이번에는 컨슈머 그룹이 다른 컨슈머들의 동작에 대해서 알려드리겠습니다.

각기 다른 컨슈머 그룹에 속한 컨슈머들은

다른 컨슈머 그룹에 영향을 미치지 않습니다.

데이터 실시간 시각화 및 분석을 위해 Elasticsearch 에 데이터를 저장하는 역할을 하는

컨슈머 그룹이 있다고 가정해봅시다.



여기에 추가로 데이터 백업 용도로 hadoop에 데이터를 저장하는 컨슈머 그룹이 새로 들어왔습니다.

만약 Elasticsearch 에 저장하는 컨슈머 그룹이

각 파티션에 특정 offset 을 읽고 있어도

hadoop 에 저장하는 역할을 하는 컨슈머 그룹이 데이터를 읽는 데는 영향을 미치지 않습니다.

![1](./img/MEMO/76.png)



왜냐하면 `__consumer_offset` 토픽에는 컨슈머 그룹별로 토픽별로 offset을 나누어 저장하기 때문이죠

이러한 카프카의 특징을 토대로 하나의 토픽으로 들어온 데이터는 

다양한 역할을 하는 컨슈머들이 각자 원하는 데이터로 처리가 될 수 있습니다.



컨슈머 그룹으로 묶인 컨슈머가 1개 이상의 토픽에서 데이터를 가져가서 처리한다는 것을 알게되었고

그리고 서로 다른 여러개의 컨슈머 그룹이 동일한 토픽에서 데이터를 가져갈 수 있다는 점도 알게 되었습니다.



또한 컨슈머가 특정 파티션의 데이터만을 가져갈 수도 있다는 점도 알게 되었습니다.

여기까지 여러분들은 카프카 브로커, 토픽, 컨슈머, 프로듀서에 대해서

모두 알게 되었습니다.





### 11강 - 카프카 스트림즈 애플리케이션

[11강 - 카프카 스트림즈! 대용량, 폭발적인 성능의 실시간 데이터 처리!](카프카 스트림즈! 대용량, 폭발적인 성능의 실시간 데이터 처리!)

<br>



### 12강 - 카프카 커넥트

[데이터 파이프라인을 가장 효율적으로 개발, 배포, 운영하는 방법! | 카프카 커넥트 Kafka Connect](https://www.youtube.com/watch?v=UURmOj6Eaoo)

<br>



## 아파치 카프카의 미래

### 13강 - 클라우드 기반 아파치 카프카 서비스

[13강 - Confluent의 Kafka cloud서비스를 소개합니다!](https://www.youtube.com/watch?v=BPdLAT6G7IE)

<br>



### 14강 - 빅데이터 플랫폼 아키텍처와 카프카

[14강 - 카파 아키텍처? 람다 아키텍처? 빅데이터 플랫폼 아키텍처의 미래 살펴보기](https://www.youtube.com/watch?v=U5G-i73Wb6U)

<br>



### 15강 - 아파치 카프카의 미래

[15강 - 아파치 카프카🚀를 알아야하는 이유! 카프카의 미래? 앞으로 어떻게될까?](https://www.youtube.com/watch?v=lEOV4upTJ68)

<br>





