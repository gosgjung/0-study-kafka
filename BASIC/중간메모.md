# 중간메모



### 1강 - 아파치 카프카 개요 및 설명

https://www.youtube.com/watch?v=waw0XXNX-uQ



### 2강 - 토픽이란?

https://www.youtube.com/watch?v=7QfEpRTRdIQ



### 3강 - 브로커, 복제, ISR(In-Sync-Replication)

3강 - [아파치 카프카 | Broker, Replication, ISR 👀핵심요소 3가지!](https://www.youtube.com/watch?v=qpEEoGpWVig)

<br>

가만히 앉아서 듣기만 하면 개꿀일줄 알았는데, 3강부터는 필기해야 머릿속에 기억에 남았었다... 역시 쉬운일이란 없다.<br>

<br>



프로듀서의 역할

토픽을 파티션에 전달한다.



브로커, 파티션

![1](./img/MEMO/1.png)

브로커는 여러 대 존재할 수 있다.

파티션은 브로커마다 여러대 설정할 수 있다.

리더 파티션을 설정하고, 다른 파티션을 팔로워 파티션으로 해서 복제본을 저장해두는 것 역시 가능하다.



#### 리더파티션과 팔로워 파티션의 역할

프로듀서가 토픽의 파티션에 데이터를 전달할 때 전달받는 주체가 Leader Partition 이다.

프로듀서에는 ack 라는 상세 옵션이 있다.

ack를 통해 고가용성을 유지할 수 있다.

이 옵션은 파티션의 레플리케이션과 관계있다.

ack 는 0, 1, all 이렇게 3개중 하나를 골라서 사용할 수 있다.



ack = 0 일 경우

- 프로듀서는 리더파티션에 데이터를 전송하고 응답값을 받지 않는다.

![1](./img/MEMO/2.png)

- 응답값을 받지 않기 때문에 Leader Partition 에 데이터가 정상적으로 전송됐는지, 나머지 파티션에 데이터가 정상적으로 복제되었는지 알수 없고, 보장할 수 없다.
- 이런 이유로 ack=0 일 때는 속도는 빠르지만, 데이터 유실 가능성이 있다.

![1](./img/MEMO/3.png)

ack=1 일 경우

- Leader Partition 에 데이터를 전송하고, Leader Partition 이 데이터를 정상적으로 받았는지 응답값을 받는다.
- 다만, 나머지 partition 에 복제되었는지는 알 수 없다.

![1](./img/MEMO/4.png)

- 만약, 리더 파티션이 데이터를 받은 직후 장애가 나면 나머지 partition 에 데이터가 미처 전송되지 못한 상태이기에 ack=0 옵션 처럼 데이터 유실 가능성이 있다.

![1](./img/MEMO/5.png)

ack=all 옵션

- 1 옵션에서 하는 기능에 follower partition 에 대한 기능이 추가된 기능
- follower partition 에 복제가 잘 이루어졌는지 응답값을 받는다.

![1](./img/MEMO/6.png)

- Leader partition 에 데이터를 보낸 후 나머지 Follower Partition 에도 데이터가 정상적으로 저장되는지 확인하는 절차를 거친다.

![1](./img/MEMO/7.png)

- ack=1, ack=0 일 때에 비해 확인하는 부분이 많기에 속도가 느리다는 단점 역시 존재한다.

<br>

#### replication 이 많을수록 좋은거 아니야?

![1](./img/MEMO/8.png)

하지만, 레플리케이션 갯수가 많아지면, 브로커가 사용하는 리소스 사용량도 같이 늘어나게 된다.

![1](./img/MEMO/9.png)

따라서 카프카에 들어오는 데이터 양과 retention date 즉, 저장시간을 잘 생각해서 replication 갯수를 잘 정하는 것이 좋다.<br>

3개 이상의 브로커를 사용할 때, replication 은 3으로 설정하는 것을 추천한다.

<BR>

### 4강 - 파티셔너란?

[4강 - 파티셔너의 역할과 동작! 파티션으로 가는 길목!](https://www.youtube.com/watch?v=-vKiNUH5OT8)

<br>

파티셔너를 알면 파티션을 효과적으로 사용할 수 있다.<br>

<br>

#### 파티셔너의 역할

프로듀서가 데이터를 보내면, 무조건 파티셔너를 통해서 브로커로 데이터가 전송된다.

파티셔너는 데이터를 어떤 파티션으로 넣을지 결정한다.

레코드에 포함된 메시지 키 or 메시지 값에 따라 파티션의 위치가 결정된다.

![1](./img/MEMO/10.png)





#### UniformStickyPartitioner

프로듀서를 사용할 때 파티셔너를 따로 설정하지 않는다면, UniformStickyPartitioner 로 설정된다. 

UniformStickyPatitioner 는 메시지 키가 있을때, 없을때 각각 다르게 동작한다.

<br>

##### 메시지 키가 있을 때

메시지 키를 가진 레코드는 파티셔너의 해시 알고리즘으로 파티션 번호를 해시값으로 생성할 수 있다.

동일한 메시지 키를 가진 레코드는 동일한 해시값을 만들어내기 때문에 항상 동일한 파티션에 들어가는 것을 보장한다.

![1](./img/MEMO/11.png)

<br>

이렇게 동일한 키를 가진 레코드 들은 동일한 파티션에 들어가는 것을 보장한다.

따라서 순서를 지켜서 데이터를 처리할 수 있다는 장점이 있다. 

(응? 강의에서 언급하는 이 부분은 조금 의미가 이상하다.  순서를 지킨다는 말은 좀 이상한데... FIFO와 혼동될법한데...) 

e.g. 

- 메시지 키 = 서울 => 파티션 0번 
- 메시지 키 = 울산 => 파티션 0번
- 메시지 키 = 부산 => 파티션 1번

예로 든 내용 역시 순서라기보다는 범위를 구분할 수 있다는 의미로 보인다.

(참고로, 파티션 한개일 경우에만 FIFO처럼 동작할 수 있는데, 실제로 이렇게 구성되는 케이스는 많지 않다.)

<BR>

##### e.g. 서울, 부산

![1](./img/MEMO/12.png)



#####  메시지 키가 없을 때

메시지 키가 없는 레코드는 라운드 로빈으로 파티션에 들어간다.

단, 전통적인 라운드 로빈 방식과는 조금 다르게 동작한다.

UniformStickyPartitioner 는 프로듀서에서 배치로 모을수 있는 최대한의 레코드를 모아서

파티션으로 데이터를 보내게 된다.

이렇게 배치 단위로 데이터를 보낼 때 파티션에 라운드 로빈 방식으로 돌아가면서 데이터를 넣게 된다.

<br>

쉽게 말해 메시지키가 없는 레코드 들은 

파티션에 적절하게 (내부적인 동작으로) 분배된다고 생각하면 될것 같습니다.



#### 커스텀 파티셔너

그럼 우리는 직접 개발한 파티셔너만 사용할 수 있을까요?

그렇지 않습니다.

직접 개발한 파티셔너도 우리가 프로듀서에서 설정할 수 있는데요.

카프카에서는 커스텀 파티셔너를 만들 수 있도록 `Partitioner` 인터페이스를 제공하고 있습니다.

`Partitioner` 인터페이스를 사용해서 커스텀 파티셔너 클래스를 만들면 

메시지 키 또는 메시지 값 또는 토픽 이름에 따라서 어느 파티션에 데이터를 보낼 것인지 정할 수 있습니다.



##### 커스텀 파티셔너를 사용하는 경우

e.g. VIP 고객을 위해서 데이터 처리를 조금 더 빠르게 하는 로직을 생각해볼 수 있을 것 같아요.

VIP 고객의 데이터를 조금 더 빠르게 처리해주고 싶다면

파티셔너를 통해서 처리량을 조금 더 늘릴수도 있습니다.

기본적으로 10개의 파티션이 있다고 가정할 때

우리가 커스텀 파티셔너를 만들어서

8개의 파티션에는 VIP 고객의 데이터를 저장하고

2개의 파티션에는 일반 고객의 데이터를 넣는 것



데이터 처리량을 조금 더 vip 고객을 위해서 몰아주는 형태로 개발할 수도 있습니다.

이것은 마치 AMQP 기반 메시징 시스템 같은 곳에서 우선순위 큐를 만드는 것과 비슷하다고 볼수 있습니다.

(응?... 흠...;;; )



### 5강 - 컨슈머 랙

[5강 - 카프카 컨슈머 Lag이란? Lag에 대해서 알아봅시다](https://www.youtube.com/watch?v=D7C_CFjrzBk)

<br>

카프카 lag 은 카프카를 모니터링할 때 중요하게 여겨지는 모니터링 지표 중 하나다.<br>

카프카 lag 이 존재하는 이유에 대해 아시려면 카프카 토픽과 파티션, 컨슈머와 프로듀서, 오프셋에 대해 모두 아셔야 합니다.<br>

만약 이 5가지에 대해서 모르신다면 [데이터💾가 저장되는 토픽에 대해서 알아봅시다.](https://www.youtube.com/watch?v=7QfEpRTRdIQ) 을 눌러서 보고 오시는 것을 추천드립니다.<br>

카프카 프로듀서는 토픽의 파티션에 데이터를 차곡 차곡 넣게 됩니다.<br>

이 파티션에 데이터가 하나 하나 들어가게 되면 각 데이터는 오프셋이라고 하는 숫자가 붙게 됩니다.<br>

<br>

e.g. 파티션이 1개일 때

만약 파티션이 1개인 토픽에 프로듀서가 데이터를 넣을 경우 0부터 차례대로 숫자가 매겨지게 됩니다.

프로듀서는 계속해서 데이터를 넣게 되고

컨슈머는 계속해서 데이터를 가져간다.

![1](./img/MEMO/13.png)



만약 프로듀서가 데이터를 넣어주는 속도가 컨슈머가 가져가는 속도보다 빠르게 되면 어떻게 될까요?

- 프로듀서가 넣은 데이터의 오프셋
- 컨슈머가 가져간 데이터의 오프셋

프로듀서의 오프셋, 컨슈머의 오프셋 이렇게 두개의 오프셋 간에 차이가 발생하게 된다.

![1](./img/MEMO/14.png)



이 lag은 적을수도 있고 많을 수도 있습니다.

이 lag의 숫자를 통해 현재 해당 토픽에 대해 파이프라인으로 연결되어 있는 프로듀서와 컨슈머의 상태에 대해 유추가 가능하게 되는데요.

주로 컨슈머의 상태에 대해 볼때 사용합니다.



lag 은 각 파티션의 오프셋 기준으로 프로듀서가 넣은 데이터의 오프셋과

컨슈머가 가져가는 데이터의 오프셋이 차이를 기반으로 합니다.



그렇기 때문에 토픽에 여러 파티션이 존재할 경우 lag은 여러개가 존재할 수 있습니다.

<br>

만약 컨슈머 그룹이 1개이고 파티션이 2개인 토픽에서 데이터를 가져간다면 lag은 2개가 측정될 수 있습니다.

![1](./img/MEMO/15.png)



이렇게 한개의 토픽과 컨슈머 그룹에 대한 lag이 여러개 존재할 수 있을 때

그 중 높은 숫자의 lag을 `records-lag-max` 라고 부릅니다.



오늘은 카프카 렉에 대해 알아봤습니다. 저는 현업에서 컨슈머 입장으로 개발을 많이 해왔기 때문에 lag에 대해 많은 모니터링 경험이 있습니다. 아무래도 consumer가 성능이 안나오거나 비정상적인 동작을 하게 되면 lag이 필연적으로 발생하기 때문에 주의깊게 살펴볼 needs 가 있기 때문입니다.



lag은 두가지만 알면 됩니다.

- 첫번째, lag은 프로듀서의 오프셋과 컨슈머의 오프셋 간의 차이다.
- 두번째, lag은 여러개가 존재할 수 있다.



### 6강 - 카프카 컨슈머 Lag 모니터링 필수 요소

- Burrow 소개 : [https://blog.voidmainvoid.net/243](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjU1UUw5OGdRd2QyQXRDOTlkenhnSFNwb2ZvQXxBQ3Jtc0tsb3F6bk92OUh6a3ctc1lwclkwNVF5UXU1Q3pZYTRpdU9hczZYc2lITDVMX3RWTGt6MERCdDVBbnVZNXJXUXZ4czhKeFNjRHlkX0g0MHVfZVp4RnBSN21mX1BLZGNUTWY3ZlFzYXgxV1FrMHZCYmxocw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F243&v=b3i6D4eeBGw) 
- Burrow의 Consumer status 확인 방법 : [https://blog.voidmainvoid.net/244](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFltMHBrZFczVkVQQkU5d1NLbHNTZVMydVBEd3xBQ3Jtc0tuZEhyaVQ0VWlnVTEyT3E0RDhIaWdRME1NWHBWUmRFQlRQMURrLUo0Y2Jad1ZjNzJrYnpfWlpxQ0VjekVMOWR2akxZRW9FLTB4cVhxS1ViWDRLaDloQV9SVENmSjFDZmNkcTBJeWRFaUZUUTFYWjFkUQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F244&v=b3i6D4eeBGw) 
- Burrow http endpoint 정리 : [https://blog.voidmainvoid.net/245](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJINUhicHptcnhRNlFwVmxBclBXODZtQTZuQXxBQ3Jtc0trYWxjd0RCVDJBYWVTcXRVWklmanJpWVl6dkJvTk4wU2RGUWQwa1dLRmgyUDFvTk9vZndjNmRhZHU1cXl5MWNJRmNnQThOYzRZM0pkXzFkbXZmcElUa1hyMmVYZFk4QWNtaGRKa005RWY1TEhhREhySQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F245&v=b3i6D4eeBGw) 
- Burrow github : [https://github.com/linkedin/Burrow](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUZOMXdwRjB3STFTcTM1MEQ4Z3FreTR2eWFwQXxBQ3Jtc0ttVjJtUUZrWFZueDMtdHNMZ1RXcnVDeFlILXpPa2NLOTYycGFmU1FnN2I5RTNyOHJ3TGpmaldCTHhLbXFGMzJnN2xJQnpDdzZUanNHZTdVLURKM0xHaTNyQ3NocU9PbU5QX014QWlFbmE0ZU41R2Vzbw&q=https%3A%2F%2Fgithub.com%2Flinkedin%2FBurrow&v=b3i6D4eeBGw)



오늘은 카프카 lag을 모니터링하기 위해 오픈소스인 Burrow 를 사용해야 하는 이유에 대해 말씀드리려 합니다.

컨슈머 Lag을 모니터링해야 하는 이유에 대해서는 

저번 영상에서 이미 말씀드렸었는데요.

아직 지난 영상을 보지 않으셨다면 [카프카 컨슈머 Lag이란? Lag에 대해서 알아봅시다](https://www.youtube.com/watch?v=D7C_CFjrzBk) 를 보고 오시는 것을 추천드립니다.



카프카 lag 은 토픽의 가장 최신 오프셋과 컨슈머 오프셋 간의 차이입니다.

![1](./img/MEMO/16.png)



kafka-client 라이브러리를 사용해 Java/Scala 같은 언어를 통해 카프카 컨슈머를 구현할 수 있는데요.

이때 구현한 카프카 컨슈머 객체를 통해 현재 lag 정보를 가져올 수 있습니다.

만약 lag을 실시간으로 모니터링하고 싶다면

데이터를 Elasticsearch 나 influxDB와 같은 저장소에 넣은 뒤

Grafana 대시보드를 통해 확인할 수도 있습니다.

![1](./img/MEMO/17.png)





그런데 문제는 이렇게 Consumer 단위에서 lag을 모니터링하는 것은 

아주 위험하고 운영요소가 많이 들어간다는 점입니다.

왜냐면 컨슈머 로직단에서 lag을 수집하는 것은

컨슈머 상태에 디펜던시가 걸리기 때문입니다.



컨슈머가 비정상적으로 종료된다면, 컨슈머는 더 이상 lag 정보를 보낼 수 없기 때문에

더이상 lag 을 측정할 수 없습니다.



그리고 추가적으로 컨슈머가 개발될 때마다 

해당 컨슈머에 lag 정보를 특정 저장소에 저장할 수 있도록 

로직을 개발해야 합니다.



만약 컨슈머 lag을 수집할 수 없는 컨슈머라면

lag을 모니터링할 수 없으므로 

운영이 매우 까다로워지게 됩니다.



그래서 linkedin 에서는 아파치 카프카와 함께

카프카 컨슈머 lag을 효과적으로 모니터링할 수 있도록

Burrow 를 내놓았습니다.



Burrow 는 오픈소스로서 Golang으로 작성되었고 현재 깃헙에 올라와있습니다.

가장 최근 릴리즈일자를 찾아보면 1월 29일 1.3.2 버전을 배포한 것을 알 수 있는데요

지속적으로 관리되고 있는 오픈소스라는 것을 알 수 있습니다.



Burrow 는 컨슈머 lag 모니터링을 도와주는 

독립적인 애플리케이션이라고 보시면 됩니다.



Burrow 는 3가지 큰 특징을 가지고 있는데요

- 1\) 멀티 카프카 클러스터 지원
  - 다양한 유즈케이스가 있을 수 있겠지만, 카프카를 운영하는 기업이라면 대부분이 2개 이상의 카프카 클러스터를 운영하고 있을 것입니다.
  - 이렇게 카프카 클러스터가 여러개이더라도 Burrow application 1개만 실행해 연동한다면 카프카 클러스터들에 붙은 컨슈머의 lag을 모두 모니터링할 수 있습니다. (참고: 아래 그림)
- 2\) Sliding window 를 통한 Consumer 의 status 확인
  - 만약 데이터 양이 일시적으로 많아지면서 consumer offset 이 증가되고 있으면 'WARNING'으로 정의됩니다.
  - 만약 데이터 양이 많아지고 있는 consumer 가 데이터를 가져가지 않으면 'ERROR'로 정의해서 실제로 컨슈머가 문제가 있는지 알 수 있습니다.
  - 이렇게 status 를 기반으로 효과적으로 운영에 참고할 수 있습니다.
- 3\) HTTP API 제공
  - 위와 같은 정보들은 Burrow 가 정의한 HTTP API 를 통해 조회할 수 있게 했습니다.
  - 세상에는 여러 프로토콜이 있지만, 그 중에 가장 범용적으로 사용되는 HTTP를 제공한 덕분에 Burrow 는 다양한 추가 생태계를 구축할 수 있게 되었습니다.
  - HTTP API를 호출해서 response 받은 데이터를 시계열 DB와 같은 곳에 저장하는 application 을 만들어서 활용할 수도 있습니다.



Burrow Application 과 카프카 클러스터들간의 관계 

![1](./img/MEMO/18.png)





아파치 카프카를 개발한 Linkedin 개발자들은 

컨슈머 lag을 어떻게 모니터링할지 많은 고민을 해왔다는 것을

Linkedin Engineering 블로그에서도 확인할 수 있습니다.



결국 아파치 카프카 외부 application 을 통해 

컨슈머 lag을 모니터링하는 것이 답이라는 것을 알게 되었고

오픈소스화된 Burrow는 현재 많은 데이터 기반 기업들이 사용 중에 있습니다.



아파치 카프카를 운영하는 기업이라면 Burrow 를 도입하지 않을 이유가 없습니다.



물론, Burrow 를 도입한다고 모든 문제가 해결되는 것은 절대 아닙니다.

다만 카프카 개발자 그리고 카프카 클러스터 운영자가 

효과적으로 카프카 관련 애플리케이션을 운영할 때 

반드시 필요한 것이고

Burrow 를 통해 수집된 데이터는

결국 추후에 애플리케이션 개발과 운영시에 많은 도움이 되기 때문입니다.



만약 이 영상을 보시는 구독자님들 중에 

회사에 아파치 카프카를 사용하고 있는데 아직 Burrow 를 도입하지 않으셨다면

이번 기회에 Burrow 를 도입하는 것을 고려해보시는 것도 

정말 좋을 것 같습니다.



- Burrow 소개 : [https://blog.voidmainvoid.net/243](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjU1UUw5OGdRd2QyQXRDOTlkenhnSFNwb2ZvQXxBQ3Jtc0tsb3F6bk92OUh6a3ctc1lwclkwNVF5UXU1Q3pZYTRpdU9hczZYc2lITDVMX3RWTGt6MERCdDVBbnVZNXJXUXZ4czhKeFNjRHlkX0g0MHVfZVp4RnBSN21mX1BLZGNUTWY3ZlFzYXgxV1FrMHZCYmxocw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F243&v=b3i6D4eeBGw) 
- Burrow의 Consumer status 확인 방법 : [https://blog.voidmainvoid.net/244](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFltMHBrZFczVkVQQkU5d1NLbHNTZVMydVBEd3xBQ3Jtc0tuZEhyaVQ0VWlnVTEyT3E0RDhIaWdRME1NWHBWUmRFQlRQMURrLUo0Y2Jad1ZjNzJrYnpfWlpxQ0VjekVMOWR2akxZRW9FLTB4cVhxS1ViWDRLaDloQV9SVENmSjFDZmNkcTBJeWRFaUZUUTFYWjFkUQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F244&v=b3i6D4eeBGw) 
- Burrow http endpoint 정리 : [https://blog.voidmainvoid.net/245](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJINUhicHptcnhRNlFwVmxBclBXODZtQTZuQXxBQ3Jtc0trYWxjd0RCVDJBYWVTcXRVWklmanJpWVl6dkJvTk4wU2RGUWQwa1dLRmgyUDFvTk9vZndjNmRhZHU1cXl5MWNJRmNnQThOYzRZM0pkXzFkbXZmcElUa1hyMmVYZFk4QWNtaGRKa005RWY1TEhhREhySQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F245&v=b3i6D4eeBGw) 
- Burrow github : [https://github.com/linkedin/Burrow](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUZOMXdwRjB3STFTcTM1MEQ4Z3FreTR2eWFwQXxBQ3Jtc0ttVjJtUUZrWFZueDMtdHNMZ1RXcnVDeFlILXpPa2NLOTYycGFmU1FnN2I5RTNyOHJ3TGpmaldCTHhLbXFGMzJnN2xJQnpDdzZUanNHZTdVLURKM0xHaTNyQ3NocU9PbU5QX014QWlFbmE0ZU41R2Vzbw&q=https%3A%2F%2Fgithub.com%2Flinkedin%2FBurrow&v=b3i6D4eeBGw)





