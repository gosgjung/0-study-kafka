# 중간메모

> 목차는 인프런 강의를 기반으로 구성.
>
> 세부적인 강의는 유튜브 강의를 더 찾아서 들으면서 정리할 예정

<br>



## 아파치 카프카 기초

### 1강 - 아파치 카프카 개요 및 설명

https://www.youtube.com/watch?v=waw0XXNX-uQ



### 2강 - 토픽이란?

https://www.youtube.com/watch?v=7QfEpRTRdIQ



### 3강 - 브로커, 복제, ISR(In-Sync-Replication)

3강 - [아파치 카프카 | Broker, Replication, ISR 👀핵심요소 3가지!](https://www.youtube.com/watch?v=qpEEoGpWVig)

<br>

가만히 앉아서 듣기만 하면 개꿀일줄 알았는데, 3강부터는 필기해야 머릿속에 기억에 남았었다... 역시 쉬운일이란 없다.<br>

<br>



프로듀서의 역할

토픽을 파티션에 전달한다.



브로커, 파티션

![1](./img/MEMO/1.png)

브로커는 여러 대 존재할 수 있다.

파티션은 브로커마다 여러대 설정할 수 있다.

리더 파티션을 설정하고, 다른 파티션을 팔로워 파티션으로 해서 복제본을 저장해두는 것 역시 가능하다.



#### 리더파티션과 팔로워 파티션의 역할

프로듀서가 토픽의 파티션에 데이터를 전달할 때 전달받는 주체가 Leader Partition 이다.

프로듀서에는 ack 라는 상세 옵션이 있다.

ack를 통해 고가용성을 유지할 수 있다.

이 옵션은 파티션의 레플리케이션과 관계있다.

ack 는 0, 1, all 이렇게 3개중 하나를 골라서 사용할 수 있다.



ack = 0 일 경우

- 프로듀서는 리더파티션에 데이터를 전송하고 응답값을 받지 않는다.

![1](./img/MEMO/2.png)

- 응답값을 받지 않기 때문에 Leader Partition 에 데이터가 정상적으로 전송됐는지, 나머지 파티션에 데이터가 정상적으로 복제되었는지 알수 없고, 보장할 수 없다.
- 이런 이유로 ack=0 일 때는 속도는 빠르지만, 데이터 유실 가능성이 있다.

![1](./img/MEMO/3.png)

ack=1 일 경우

- Leader Partition 에 데이터를 전송하고, Leader Partition 이 데이터를 정상적으로 받았는지 응답값을 받는다.
- 다만, 나머지 partition 에 복제되었는지는 알 수 없다.

![1](./img/MEMO/4.png)

- 만약, 리더 파티션이 데이터를 받은 직후 장애가 나면 나머지 partition 에 데이터가 미처 전송되지 못한 상태이기에 ack=0 옵션 처럼 데이터 유실 가능성이 있다.

![1](./img/MEMO/5.png)

ack=all 옵션

- 1 옵션에서 하는 기능에 follower partition 에 대한 기능이 추가된 기능
- follower partition 에 복제가 잘 이루어졌는지 응답값을 받는다.

![1](./img/MEMO/6.png)

- Leader partition 에 데이터를 보낸 후 나머지 Follower Partition 에도 데이터가 정상적으로 저장되는지 확인하는 절차를 거친다.

![1](./img/MEMO/7.png)

- ack=1, ack=0 일 때에 비해 확인하는 부분이 많기에 속도가 느리다는 단점 역시 존재한다.

<br>

#### replication 이 많을수록 좋은거 아니야?

![1](./img/MEMO/8.png)

하지만, 레플리케이션 갯수가 많아지면, 브로커가 사용하는 리소스 사용량도 같이 늘어나게 된다.

![1](./img/MEMO/9.png)

따라서 카프카에 들어오는 데이터 양과 retention date 즉, 저장시간을 잘 생각해서 replication 갯수를 잘 정하는 것이 좋다.<br>

3개 이상의 브로커를 사용할 때, replication 은 3으로 설정하는 것을 추천한다.

<BR>

### 4강 - 파티셔너란?

[4강 - 파티셔너의 역할과 동작! 파티션으로 가는 길목!](https://www.youtube.com/watch?v=-vKiNUH5OT8)

<br>

파티셔너를 알면 파티션을 효과적으로 사용할 수 있다.<br>

<br>

#### 파티셔너의 역할

프로듀서가 데이터를 보내면, 무조건 파티셔너를 통해서 브로커로 데이터가 전송된다.

파티셔너는 데이터를 어떤 파티션으로 넣을지 결정한다.

레코드에 포함된 메시지 키 or 메시지 값에 따라 파티션의 위치가 결정된다.

![1](./img/MEMO/10.png)





#### UniformStickyPartitioner

프로듀서를 사용할 때 파티셔너를 따로 설정하지 않는다면, UniformStickyPartitioner 로 설정된다. 

UniformStickyPatitioner 는 메시지 키가 있을때, 없을때 각각 다르게 동작한다.

<br>

##### 메시지 키가 있을 때

메시지 키를 가진 레코드는 파티셔너의 해시 알고리즘으로 파티션 번호를 해시값으로 생성할 수 있다.

동일한 메시지 키를 가진 레코드는 동일한 해시값을 만들어내기 때문에 항상 동일한 파티션에 들어가는 것을 보장한다.

![1](./img/MEMO/11.png)

<br>

이렇게 동일한 키를 가진 레코드 들은 동일한 파티션에 들어가는 것을 보장한다.

따라서 순서를 지켜서 데이터를 처리할 수 있다는 장점이 있다. 

(응? 강의에서 언급하는 이 부분은 조금 의미가 이상하다.  순서를 지킨다는 말은 좀 이상한데... FIFO와 혼동될법한데...) 

e.g. 

- 메시지 키 = 서울 => 파티션 0번 
- 메시지 키 = 울산 => 파티션 0번
- 메시지 키 = 부산 => 파티션 1번

예로 든 내용 역시 순서라기보다는 범위를 구분할 수 있다는 의미로 보인다.

(참고로, 파티션 한개일 경우에만 FIFO처럼 동작할 수 있는데, 실제로 이렇게 구성되는 케이스는 많지 않다.)

<BR>

##### e.g. 서울, 부산

![1](./img/MEMO/12.png)



#####  메시지 키가 없을 때

메시지 키가 없는 레코드는 라운드 로빈으로 파티션에 들어간다.

단, 전통적인 라운드 로빈 방식과는 조금 다르게 동작한다.

UniformStickyPartitioner 는 프로듀서에서 배치로 모을수 있는 최대한의 레코드를 모아서

파티션으로 데이터를 보내게 된다.

이렇게 배치 단위로 데이터를 보낼 때 파티션에 라운드 로빈 방식으로 돌아가면서 데이터를 넣게 된다.

<br>

쉽게 말해 메시지키가 없는 레코드 들은 

파티션에 적절하게 (내부적인 동작으로) 분배된다고 생각하면 될것 같습니다.



#### 커스텀 파티셔너

그럼 우리는 직접 개발한 파티셔너만 사용할 수 있을까요?

그렇지 않습니다.

직접 개발한 파티셔너도 우리가 프로듀서에서 설정할 수 있는데요.

카프카에서는 커스텀 파티셔너를 만들 수 있도록 `Partitioner` 인터페이스를 제공하고 있습니다.

`Partitioner` 인터페이스를 사용해서 커스텀 파티셔너 클래스를 만들면 

메시지 키 또는 메시지 값 또는 토픽 이름에 따라서 어느 파티션에 데이터를 보낼 것인지 정할 수 있습니다.



##### 커스텀 파티셔너를 사용하는 경우

e.g. VIP 고객을 위해서 데이터 처리를 조금 더 빠르게 하는 로직을 생각해볼 수 있을 것 같아요.

VIP 고객의 데이터를 조금 더 빠르게 처리해주고 싶다면

파티셔너를 통해서 처리량을 조금 더 늘릴수도 있습니다.

기본적으로 10개의 파티션이 있다고 가정할 때

우리가 커스텀 파티셔너를 만들어서

8개의 파티션에는 VIP 고객의 데이터를 저장하고

2개의 파티션에는 일반 고객의 데이터를 넣는 것



데이터 처리량을 조금 더 vip 고객을 위해서 몰아주는 형태로 개발할 수도 있습니다.

이것은 마치 AMQP 기반 메시징 시스템 같은 곳에서 우선순위 큐를 만드는 것과 비슷하다고 볼수 있습니다.

(응?... 흠...;;; )



### 5강 - 컨슈머 랙

[5강 - 카프카 컨슈머 Lag이란? Lag에 대해서 알아봅시다](https://www.youtube.com/watch?v=D7C_CFjrzBk)

<br>

카프카 lag 은 카프카를 모니터링할 때 중요하게 여겨지는 모니터링 지표 중 하나다.<br>

카프카 lag 이 존재하는 이유에 대해 아시려면 카프카 토픽과 파티션, 컨슈머와 프로듀서, 오프셋에 대해 모두 아셔야 합니다.<br>

만약 이 5가지에 대해서 모르신다면 [데이터💾가 저장되는 토픽에 대해서 알아봅시다.](https://www.youtube.com/watch?v=7QfEpRTRdIQ) 을 눌러서 보고 오시는 것을 추천드립니다.<br>

카프카 프로듀서는 토픽의 파티션에 데이터를 차곡 차곡 넣게 됩니다.<br>

이 파티션에 데이터가 하나 하나 들어가게 되면 각 데이터는 오프셋이라고 하는 숫자가 붙게 됩니다.<br>

<br>

e.g. 파티션이 1개일 때

만약 파티션이 1개인 토픽에 프로듀서가 데이터를 넣을 경우 0부터 차례대로 숫자가 매겨지게 됩니다.

프로듀서는 계속해서 데이터를 넣게 되고

컨슈머는 계속해서 데이터를 가져간다.

![1](./img/MEMO/13.png)



만약 프로듀서가 데이터를 넣어주는 속도가 컨슈머가 가져가는 속도보다 빠르게 되면 어떻게 될까요?

- 프로듀서가 넣은 데이터의 오프셋
- 컨슈머가 가져간 데이터의 오프셋

프로듀서의 오프셋, 컨슈머의 오프셋 이렇게 두개의 오프셋 간에 차이가 발생하게 된다.

![1](./img/MEMO/14.png)



이 lag은 적을수도 있고 많을 수도 있습니다.

이 lag의 숫자를 통해 현재 해당 토픽에 대해 파이프라인으로 연결되어 있는 프로듀서와 컨슈머의 상태에 대해 유추가 가능하게 되는데요.

주로 컨슈머의 상태에 대해 볼때 사용합니다.



lag 은 각 파티션의 오프셋 기준으로 프로듀서가 넣은 데이터의 오프셋과

컨슈머가 가져가는 데이터의 오프셋이 차이를 기반으로 합니다.



그렇기 때문에 토픽에 여러 파티션이 존재할 경우 lag은 여러개가 존재할 수 있습니다.

<br>

만약 컨슈머 그룹이 1개이고 파티션이 2개인 토픽에서 데이터를 가져간다면 lag은 2개가 측정될 수 있습니다.

![1](./img/MEMO/15.png)



이렇게 한개의 토픽과 컨슈머 그룹에 대한 lag이 여러개 존재할 수 있을 때

그 중 높은 숫자의 lag을 `records-lag-max` 라고 부릅니다.



오늘은 카프카 렉에 대해 알아봤습니다. 저는 현업에서 컨슈머 입장으로 개발을 많이 해왔기 때문에 lag에 대해 많은 모니터링 경험이 있습니다. 아무래도 consumer가 성능이 안나오거나 비정상적인 동작을 하게 되면 lag이 필연적으로 발생하기 때문에 주의깊게 살펴볼 needs 가 있기 때문입니다.



lag은 두가지만 알면 됩니다.

- 첫번째, lag은 프로듀서의 오프셋과 컨슈머의 오프셋 간의 차이다.
- 두번째, lag은 여러개가 존재할 수 있다.



### 6강 - 카프카 컨슈머 Lag 모니터링 필수 요소

- Burrow 소개 : [https://blog.voidmainvoid.net/243](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjU1UUw5OGdRd2QyQXRDOTlkenhnSFNwb2ZvQXxBQ3Jtc0tsb3F6bk92OUh6a3ctc1lwclkwNVF5UXU1Q3pZYTRpdU9hczZYc2lITDVMX3RWTGt6MERCdDVBbnVZNXJXUXZ4czhKeFNjRHlkX0g0MHVfZVp4RnBSN21mX1BLZGNUTWY3ZlFzYXgxV1FrMHZCYmxocw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F243&v=b3i6D4eeBGw) 
- Burrow의 Consumer status 확인 방법 : [https://blog.voidmainvoid.net/244](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFltMHBrZFczVkVQQkU5d1NLbHNTZVMydVBEd3xBQ3Jtc0tuZEhyaVQ0VWlnVTEyT3E0RDhIaWdRME1NWHBWUmRFQlRQMURrLUo0Y2Jad1ZjNzJrYnpfWlpxQ0VjekVMOWR2akxZRW9FLTB4cVhxS1ViWDRLaDloQV9SVENmSjFDZmNkcTBJeWRFaUZUUTFYWjFkUQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F244&v=b3i6D4eeBGw) 
- Burrow http endpoint 정리 : [https://blog.voidmainvoid.net/245](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJINUhicHptcnhRNlFwVmxBclBXODZtQTZuQXxBQ3Jtc0trYWxjd0RCVDJBYWVTcXRVWklmanJpWVl6dkJvTk4wU2RGUWQwa1dLRmgyUDFvTk9vZndjNmRhZHU1cXl5MWNJRmNnQThOYzRZM0pkXzFkbXZmcElUa1hyMmVYZFk4QWNtaGRKa005RWY1TEhhREhySQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F245&v=b3i6D4eeBGw) 
- Burrow github : [https://github.com/linkedin/Burrow](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUZOMXdwRjB3STFTcTM1MEQ4Z3FreTR2eWFwQXxBQ3Jtc0ttVjJtUUZrWFZueDMtdHNMZ1RXcnVDeFlILXpPa2NLOTYycGFmU1FnN2I5RTNyOHJ3TGpmaldCTHhLbXFGMzJnN2xJQnpDdzZUanNHZTdVLURKM0xHaTNyQ3NocU9PbU5QX014QWlFbmE0ZU41R2Vzbw&q=https%3A%2F%2Fgithub.com%2Flinkedin%2FBurrow&v=b3i6D4eeBGw)



오늘은 카프카 lag을 모니터링하기 위해 오픈소스인 Burrow 를 사용해야 하는 이유에 대해 말씀드리려 합니다.

컨슈머 Lag을 모니터링해야 하는 이유에 대해서는 

저번 영상에서 이미 말씀드렸었는데요.

아직 지난 영상을 보지 않으셨다면 [카프카 컨슈머 Lag이란? Lag에 대해서 알아봅시다](https://www.youtube.com/watch?v=D7C_CFjrzBk) 를 보고 오시는 것을 추천드립니다.



카프카 lag 은 토픽의 가장 최신 오프셋과 컨슈머 오프셋 간의 차이입니다.

![1](./img/MEMO/16.png)



kafka-client 라이브러리를 사용해 Java/Scala 같은 언어를 통해 카프카 컨슈머를 구현할 수 있는데요.

이때 구현한 카프카 컨슈머 객체를 통해 현재 lag 정보를 가져올 수 있습니다.

만약 lag을 실시간으로 모니터링하고 싶다면

데이터를 Elasticsearch 나 influxDB와 같은 저장소에 넣은 뒤

Grafana 대시보드를 통해 확인할 수도 있습니다.

![1](./img/MEMO/17.png)





그런데 문제는 이렇게 Consumer 단위에서 lag을 모니터링하는 것은 

아주 위험하고 운영요소가 많이 들어간다는 점입니다.

왜냐면 컨슈머 로직단에서 lag을 수집하는 것은

컨슈머 상태에 디펜던시가 걸리기 때문입니다.



컨슈머가 비정상적으로 종료된다면, 컨슈머는 더 이상 lag 정보를 보낼 수 없기 때문에

더이상 lag 을 측정할 수 없습니다.



그리고 추가적으로 컨슈머가 개발될 때마다 

해당 컨슈머에 lag 정보를 특정 저장소에 저장할 수 있도록 

로직을 개발해야 합니다.



만약 컨슈머 lag을 수집할 수 없는 컨슈머라면

lag을 모니터링할 수 없으므로 

운영이 매우 까다로워지게 됩니다.



그래서 linkedin 에서는 아파치 카프카와 함께

카프카 컨슈머 lag을 효과적으로 모니터링할 수 있도록

Burrow 를 내놓았습니다.



Burrow 는 오픈소스로서 Golang으로 작성되었고 현재 깃헙에 올라와있습니다.

가장 최근 릴리즈일자를 찾아보면 1월 29일 1.3.2 버전을 배포한 것을 알 수 있는데요

지속적으로 관리되고 있는 오픈소스라는 것을 알 수 있습니다.



Burrow 는 컨슈머 lag 모니터링을 도와주는 

독립적인 애플리케이션이라고 보시면 됩니다.



Burrow 는 3가지 큰 특징을 가지고 있는데요

- 1\) 멀티 카프카 클러스터 지원
  - 다양한 유즈케이스가 있을 수 있겠지만, 카프카를 운영하는 기업이라면 대부분이 2개 이상의 카프카 클러스터를 운영하고 있을 것입니다.
  - 이렇게 카프카 클러스터가 여러개이더라도 Burrow application 1개만 실행해 연동한다면 카프카 클러스터들에 붙은 컨슈머의 lag을 모두 모니터링할 수 있습니다. (참고: 아래 그림)
- 2\) Sliding window 를 통한 Consumer 의 status 확인
  - 만약 데이터 양이 일시적으로 많아지면서 consumer offset 이 증가되고 있으면 'WARNING'으로 정의됩니다.
  - 만약 데이터 양이 많아지고 있는 consumer 가 데이터를 가져가지 않으면 'ERROR'로 정의해서 실제로 컨슈머가 문제가 있는지 알 수 있습니다.
  - 이렇게 status 를 기반으로 효과적으로 운영에 참고할 수 있습니다.
- 3\) HTTP API 제공
  - 위와 같은 정보들은 Burrow 가 정의한 HTTP API 를 통해 조회할 수 있게 했습니다.
  - 세상에는 여러 프로토콜이 있지만, 그 중에 가장 범용적으로 사용되는 HTTP를 제공한 덕분에 Burrow 는 다양한 추가 생태계를 구축할 수 있게 되었습니다.
  - HTTP API를 호출해서 response 받은 데이터를 시계열 DB와 같은 곳에 저장하는 application 을 만들어서 활용할 수도 있습니다.



Burrow Application 과 카프카 클러스터들간의 관계 

![1](./img/MEMO/18.png)





아파치 카프카를 개발한 Linkedin 개발자들은 

컨슈머 lag을 어떻게 모니터링할지 많은 고민을 해왔다는 것을

Linkedin Engineering 블로그에서도 확인할 수 있습니다.



결국 아파치 카프카 외부 application 을 통해 

컨슈머 lag을 모니터링하는 것이 답이라는 것을 알게 되었고

오픈소스화된 Burrow는 현재 많은 데이터 기반 기업들이 사용 중에 있습니다.



아파치 카프카를 운영하는 기업이라면 Burrow 를 도입하지 않을 이유가 없습니다.



물론, Burrow 를 도입한다고 모든 문제가 해결되는 것은 절대 아닙니다.

다만 카프카 개발자 그리고 카프카 클러스터 운영자가 

효과적으로 카프카 관련 애플리케이션을 운영할 때 

반드시 필요한 것이고

Burrow 를 통해 수집된 데이터는

결국 추후에 애플리케이션 개발과 운영시에 많은 도움이 되기 때문입니다.



만약 이 영상을 보시는 구독자님들 중에 

회사에 아파치 카프카를 사용하고 있는데 아직 Burrow 를 도입하지 않으셨다면

이번 기회에 Burrow 를 도입하는 것을 고려해보시는 것도 

정말 좋을 것 같습니다.



- Burrow 소개 : [https://blog.voidmainvoid.net/243](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjU1UUw5OGdRd2QyQXRDOTlkenhnSFNwb2ZvQXxBQ3Jtc0tsb3F6bk92OUh6a3ctc1lwclkwNVF5UXU1Q3pZYTRpdU9hczZYc2lITDVMX3RWTGt6MERCdDVBbnVZNXJXUXZ4czhKeFNjRHlkX0g0MHVfZVp4RnBSN21mX1BLZGNUTWY3ZlFzYXgxV1FrMHZCYmxocw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F243&v=b3i6D4eeBGw) 
- Burrow의 Consumer status 확인 방법 : [https://blog.voidmainvoid.net/244](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFltMHBrZFczVkVQQkU5d1NLbHNTZVMydVBEd3xBQ3Jtc0tuZEhyaVQ0VWlnVTEyT3E0RDhIaWdRME1NWHBWUmRFQlRQMURrLUo0Y2Jad1ZjNzJrYnpfWlpxQ0VjekVMOWR2akxZRW9FLTB4cVhxS1ViWDRLaDloQV9SVENmSjFDZmNkcTBJeWRFaUZUUTFYWjFkUQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F244&v=b3i6D4eeBGw) 
- Burrow http endpoint 정리 : [https://blog.voidmainvoid.net/245](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWJINUhicHptcnhRNlFwVmxBclBXODZtQTZuQXxBQ3Jtc0trYWxjd0RCVDJBYWVTcXRVWklmanJpWVl6dkJvTk4wU2RGUWQwa1dLRmgyUDFvTk9vZndjNmRhZHU1cXl5MWNJRmNnQThOYzRZM0pkXzFkbXZmcElUa1hyMmVYZFk4QWNtaGRKa005RWY1TEhhREhySQ&q=https%3A%2F%2Fblog.voidmainvoid.net%2F245&v=b3i6D4eeBGw) 
- Burrow github : [https://github.com/linkedin/Burrow](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUZOMXdwRjB3STFTcTM1MEQ4Z3FreTR2eWFwQXxBQ3Jtc0ttVjJtUUZrWFZueDMtdHNMZ1RXcnVDeFlILXpPa2NLOTYycGFmU1FnN2I5RTNyOHJ3TGpmaldCTHhLbXFGMzJnN2xJQnpDdzZUanNHZTdVLURKM0xHaTNyQ3NocU9PbU5QX014QWlFbmE0ZU41R2Vzbw&q=https%3A%2F%2Fgithub.com%2Flinkedin%2FBurrow&v=b3i6D4eeBGw)



### 7강 - 카프카, 레빗엠큐, 레디스 큐의 차이점

[7강 - 카프카, 레빗엠큐, 레디스 큐의 큰 차이점! 이벤트 브로커와 메시지 브로커에 대해 알아봅시다.](https://www.youtube.com/watch?v=H_DaPyUOeTo)



흔히 말하는 이 메시징 플랫폼이라고 부르는 것들은 사실

두가지 종류로 나누어지는데요.



첫번째는 메시지 브로커이고

두번째는 이벤트 브로커에요



이 둘의 관계에 대해서 짧게 설명드리자면

메시지 브로커는 이벤트 브로커로 역할을 할 수 없지만

이벤트 브로커는 메시지 브로커로 역할을 할 수 있습니다.



이 설명으로는 아마 충분히 이해하기 어려울 수 있어요



먼저, 첫번째로 메시지 브로커부터 알아볼께요

우선 메시지 브로커는 많은 기업들에서 대규모 메시지 기반 미들웨어 아키텍처에서 사용되어 왔습니다.

미들웨어라는 것은 서비스하는 애플리케이션들을 

보다 효율적으로 아키텍처들을 연결하는 요소들로 작동하는 소프트웨어를 뜻하는데요.



메시징 플랫폼, 인증 플랫폼, 데이터베이스 를 미들웨어라고 볼 수 있어요

메시지 브로커에 있는 큐에 데이터를 보내고 받는 

프로듀서와 컨슈머를 통해 메시지를 통신하고 네트워크를 맺는 용도로 사용해왔죠.



메시지 브로커의 특징으로는

메시지를 받아서 적절히 처리하고 나면

즉시 또는 짧은 시간 내에 삭제되는 구조입니다.



반면에 이벤트 브로커는 메시지 브로커의 특징과

조금 다른 구조로 만들어져 있는데요.



두가지 큰 특징이 있어요



첫번째는 이벤트 또는 메시지라고 불리는 이 레코드 이 장부를 딱 하나만 보관하고 

인덱스를 통해 개별 액세스를 관리합니다.



두번째는 업무상 필요한 시간동안 이벤트를 보존할 수 있습니다.

확실히 메시지 브로커랑 결이 다른 것이 느껴지시나요?



메시지 브로커는 데이터를 보내고 처리하고 삭제한다.

그런데, 이벤트 브로커는 삭제하지 않아요



아니, 데이터를 처리했는데 왜 삭제하지 않는 걸까요?

단서는 '이벤트'라는 단어에 숨어있습니다.



이벤트 브로커는 서비스에서 나오는 이벤트를 

마치 데이터베이스에 저장하듯이

이벤트 브로커의 큐에 저장하는 데요

이렇게 저장함으로써 얻는 명확한 이점이 있습니다.



첫 번째 이점은 딱 한번 일어난 이벤트 데이터를 브로커에 저장함으로써 단일 진실 공급원으로 사용할 수 있구요.

두 번째는 장애가 발생했을 때, 장애가 일어난 지점부터 재처리할 수 있습니다.

그리고 세번째로는 많은 양의 실시간 스트림 데이터를 효과적으로 처리할 수 있다는 특징이 있어요.



그 외에도 다양한 이벤트 기반 마이크로 서비스 아키텍처에서 중요한 역할을 맡을 수 있습니다.

메시지 브로커는 보통 레디스 큐나 레빗엠큐 같은 것을 예로 들 수 있고

이벤트 브로커는 카프카나 AWS의 키네시스가 대표적이겠죠.



이벤트 브로커로 클러스터를 구축하면

이벤트 기반 마이크로 서비스 아키텍처로 발전하는데

아주 중요한 역할을 할 뿐만 아니라

메시지 브로커로서도 사용할 수 있으니까

정말 팔방 미인이 아닐 수 없어요



여기까지 이벤트 브로커와 메시지 브로커에 대한 차이점

그리고 각각의 특징에 대해서 간략하게 살펴보았는데요.



## 아파치 카프카 개발

### 8강 - AWS에 카프카 클러스터 설치, 실행하기

[8강 - AWS에 카프카 클러스터를 설치, 실행해보기!](https://www.youtube.com/watch?v=Qr0HVvtMFhg)

자세한 설치 과정은 [블로그](https://blog.voidmainvoid.net/325)를 참고<br>

<br>

이 영상을 보시는 많은 개발자 분들은 이미 자신의 컴퓨터에 1개의 node로 카프카 브로커를

homebrew를 통해 설치해보셨을 것 같은데요.

사실 3개 이상의 카프카 브로커로 이루어진 클러스터를 구축하고 사용해야지만

정말로 카프카를 써봤다 라고 말할 수 있을 것 같습니다.

왜냐면 카프카의 고가용성의 핵심은 3개 이상의 카프카 브로커들로 이루어진

클러스터에서 진가를 발휘하게 되거든요



그래서 오늘은 aws 의 ec2 서버 3대를 발급받아서

카프카를 설치해보고 console producer와 console consumer 로 

연동해보는 실습을 해보도록 하겠습니다.



먼저 아파치 카프카를 설치하기 위해서는 2가지의 애플리케이션이 필요한데요.

먼저 첫번째는 주키퍼이고, 두번째는 카프카입니다.

주키퍼는 카프카 관련 정보를 저장하는 역할을 하는데요

카프카에서 주키퍼의 역할에 대해서는 다른 영상에서 자세히 설명드리도록 하겠습니다.



그럼 한번 카프카와 주키퍼 설치를 진행해보도록 하겠습니다.



테스트목적의 머신이므로 1cpu, 1g ram 으로 구성된 t2.micro 를 발급받습니다.

![1](./img/MEMO/19.png)



카프카 클러스터를 최소로 구축하기 위해 3대의 인스턴스를 발급받습니다.

![1](./img/MEMO/20.png)



키페어 다운로드

![1](./img/MEMO/21.png)



이제 인스턴스 시작 버튼을 눌러 서버가 실행될 때 까지 기다립니다.

![1](./img/MEMO/22.png)



인스턴스가 모두 시작되고 나면 인스턴스에 접속하셔서 wget 명령어로 주키퍼를 다운받습니다.

![1](./img/MEMO/23.png)



다운받은 주키퍼의 압축을 풀어줍니다.

![1](./img/MEMO/24.png)



주키퍼 앙상블을 구축하기 위해서는 각 서버마다 주키퍼 설정을 하도록 합니다.

![1](./img/MEMO/25.png)



자세한 주키퍼 설정은 영상하단의 블로그 링크에서도 확인하실 수 있습니다.

![1](./img/MEMO/26.png)



각 서버별로 ip가 아닌 hostname으로 통신하기 위해 /etc/host 를 수정해줍니다.

![1](./img/MEMO/27.png)



주키퍼를 서로 연동하기 위해서는 방화벽 설정이 필요한데요

방화벽 설정은 시큐리티 그룹의 inbound rule과 outbound rule 을 통해 설정 가능합니다.

zookeeper 는 2181 포트와 2888포트, 3888 포트를 사용하므로

3개의 포트에 대해서 anywhere 조건으로 open 하도록 하겠습니다.



인바운드 방화벽 설정을 수정합니다.

![1](./img/MEMO/28.png)



참고로 카프카 통신을 위해 9092 포트도 열어주시면 좋습니다.

![1](./img/MEMO/29.png)



이제 설정이 완료되었으니 각 서버에서 주키퍼를 실행합니다.

![1](./img/MEMO/30.png)



주키퍼가 정상적으로 설치되었고 방화벽이 정상이라면

local 컴퓨터의 주키퍼 cli 를 통해

aws 에서 실행되는 주키퍼에 연결하실 수 있습니다.

![1](./img/MEMO/32.png)



이제 카프카를 설치해보도록 하겠습니다.

이번에 설치할 카프카는 2.1.0 버전인데요.

가장 최근에 나온 버전인 2.4.0 보다 좀 더 낮은 버전입니다.



카프카 압축 파일을 인터넷에서 다운로드합니다.

![1](./img/MEMO/33.png)





카프카 클러스터를 구성하기 위해 각 브로커 별로 카프카 설정이 필요합니다.

![1](./img/MEMO/34.png)



가장 중요한 broker.id 를 각 서버별로 각각 다른 숫자로 설정해줍니다.

listener 와 advertise listener 도 설정합니다.(그림 생략)

![1](./img/MEMO/35.png)



그리고 아까 실행한 주키퍼의 hostname과 port 도 넣어주도록 합니다.

![1](./img/MEMO/36.png)



이제 카프카를 실행해보도록 하겠습니다.

![1](./img/MEMO/37.png)





카프카 실행 된 후 테스트를 위해 `test_log` 라고 하는 토픽을 만들어보겠습니다.

![1](./img/MEMO/38.png)



그리고 만들어진 토픽에 console-producer 로 데이터를 넣습니다.

![1](./img/MEMO/39.png)



동시에 console-consumer로 데이터를 확인합니다.

![1](./img/MEMO/40.png)



AWS에 설치된 카프카와 정상적으로 통신하는 것을 확인할 수 있습니다.

![1](./img/MEMO/41.png)



여기까지 카프카 클러스터를 멀티 브로커로 구축해보고

local 컴퓨터에서 카프카 클러스터로

연동해보는 실습까지 완료했습니다.



이렇게 만들어진 카프카 클러스터는 3개의 브로커로 이루어져 있기 때문에

고 가용성을 만족한다고 볼 수 있고

고 가용성을 만족하는 훌륭한 클러스터를 

직접 만드셨다는 것에 자부심을 가져도 될 것 같습니다.



오늘 설치과정은 [블로그](https://blog.voidmainvoid.net/325) 에서 확인할 수 있습니다.



### 9강 - 카프카 프로듀서 애플리케이션

[9강 - 데이터를 카프카로 전송🚀하는 프로듀서](https://www.youtube.com/watch?v=aAu0FE3nvbk)

<br>



### 10강 - 카프카 컨슈머 애플리케이션

[10강 - 카프카 컨슈머 역할 및 코드예제](https://www.youtube.com/watch?v=rBVCvv9skT4)

<br>



### 11강 - 카프카 스트림즈 애플리케이션

[11강 - 카프카 스트림즈! 대용량, 폭발적인 성능의 실시간 데이터 처리!](카프카 스트림즈! 대용량, 폭발적인 성능의 실시간 데이터 처리!)

<br>



### 12강 - 카프카 커넥트

[데이터 파이프라인을 가장 효율적으로 개발, 배포, 운영하는 방법! | 카프카 커넥트 Kafka Connect](https://www.youtube.com/watch?v=UURmOj6Eaoo)

<br>



## 아파치 카프카의 미래

### 13강 - 클라우드 기반 아파치 카프카 서비스

[13강 - Confluent의 Kafka cloud서비스를 소개합니다!](https://www.youtube.com/watch?v=BPdLAT6G7IE)

<br>



### 14강 - 빅데이터 플랫폼 아키텍처와 카프카

[14강 - 카파 아키텍처? 람다 아키텍처? 빅데이터 플랫폼 아키텍처의 미래 살펴보기](https://www.youtube.com/watch?v=U5G-i73Wb6U)

<br>



### 15강 - 아파치 카프카의 미래

[15강 - 아파치 카프카🚀를 알아야하는 이유! 카프카의 미래? 앞으로 어떻게될까?](https://www.youtube.com/watch?v=lEOV4upTJ68)

<br>





